[["index.html", "Introduction to Computational Neuroscience 1 Introduction to the book", " Introduction to Computational Neuroscience Matthew J. Crossley 2024-02-13 1 Introduction to the book Put intro here. "],["introduction-to-the-book-1.html", "2 Introduction to the book 2.1 What is computational neuroscience? 2.2 Marr’s levels 2.3 Why computers? 2.4 What is the real value gained? 2.5 Why Python 2.6 Getting started with Python 2.7 Learning Python", " 2 Introduction to the book 2.1 What is computational neuroscience? Computational neuroscience is just using computers to study brains. What counts as brains has come to refer to pretty much anything across a very wide range of levels. E.g., Biochemical and electrical signal transduction and prorogation within individual cell compartments. Intracellular signalling cascades. Whole cell action potential spiking properties. Collection of neurons as dynamical systems. Local field potential, hemodynamic response, cortical oscillatory patterns. Behaviour (e.g., decision making, action selection and execution). Using computers to study purely neural systems – without concern for behaviour – is pretty fairly called computational neuroscience. Using computers to study behaviour – without concern for neural systems – would fall more naturally under the banner of cognitive modelling. Using computers to study how the brain drives behaviour – i.e., explicitly caring about both neural and cognitive domains and trying to link them – is sometimes given special names like computational cognitive neuroscience. 2.2 Marr’s levels Computational: A specification of what a system does and why it does it. Algorithmic: A specification of how it does what it does (i.e., what process is followed). implementational: A specification of how the system is implemented in the brain. In my opinion, few models cleanly reside at a single one of these levels and many probably are best classified between levels. Even so, the conceptual divisions they get us thinking about are useful to have on hand. 2.3 Why computers? Mathematics is at the heart of computational neuroscience. The goal is to write down equations that formally describe the process through which the phenomena of interest are hypothesised to emerge. The need for computers arises when the mathematics that you write down is complicated and difficult to evaluate. Computational approaches give us tools to deal with these difficult mathematical situations. Well, some of them at least. 2.4 What is the real value gained? Have you ever stuggled or argued with yourelf or others about what a certain hypothesis predicts in a particular experiment? Mathematical precision makes science happy. It eliminates ambiguity and thereby makes mathematical models more falsifiable. In practice, the word more in the last sentence above can do an awful lot of work. Coming up with experiments that are strong tests of any model, even mathematically precise models, can be very difficult. Even so, there is no question that mathematics and computers help… and sometimes they help a whole lot. 2.5 Why Python It is a real programming language It is mature It is widely embraced in psychology and neuroscience It is widely embraced inside and outside academia It is widely used in the machine learning community It is relatively easy to learn 2.6 Getting started with Python I recommend getting started with Google Colab. If you dislike the web notebook interface, then you might try Anaconda. Anaconda will provide the IDE Spyder which isn’t a bad place to work with Python. Ultimately, there are many Python programming tools to choose from. Use what you like. 2.7 Learning Python Just like with other programming languages you have been exposed to, the base functionality of Python is extended by external libraries. We will make heavy use of the following libraries: Python Learn the basics Learn the basics 2 This one isn’t as beginner friendly. It’s just here for those of you with more programming experience that want to advance your skills and understand the language at a deeper level. Numpy Learn Numpy This will be the focus of week 2 Matplotlib Learn Matplotlib This will be the focus of weeks 2 and 3 "],["numpy-and-matplotlib.html", "3 Numpy and Matplotlib 3.1 Learning Python", " 3 Numpy and Matplotlib 3.1 Learning Python Just like with other programming languages you have been exposed to, the base functionality of Python is extended by external libraries. We will make heavy use of the following libraries: Numpy Learn Numpy Matplotlib Learn Matplotlib "],["intro-to-calculus.html", "4 Intro to calculus", " 4 Intro to calculus TODO: "],["intro-to-dynamical-systems.html", "5 Intro to dynamical systems 5.1 Differential equations 5.2 Euler’s method", " 5 Intro to dynamical systems 5.1 Differential equations The HH model and lots of other models we will encounter are ultimately expressed in differential equations. A differential equation is essentially an equation that relates some function \\(f(x)\\) to its derivative \\(\\frac{d}{dx}f(x)\\) The derivative \\(\\frac{d}{dx}f(x)\\) is the rate of change of the function \\(f(x)\\) with respect to the variable \\(x\\). If \\(x\\) changes by an infinitesimal amount, \\(\\frac{d}{dx}f(x)\\) reports how much \\(f(x)\\) will change in response. A generic differential equation is as follows: \\[\\frac{d}{dx}f(x) = g(x)\\] You can read this in words as saying that the rate of change of \\(f(x)\\) with respect to \\(x\\) — given by \\(\\frac{d}{dx}f(x)\\) — is described by some other function \\(g(x)\\). To solve a differential equation, we need to find a definition for \\(f(x)\\) that makes the equation true. For example, if \\(g(x)=x\\) then: \\[\\frac{d}{dx}f(x) = x\\] We can see that \\(f(x) = \\frac{1}{2}x^2\\) solves the differential equation, because \\[ \\begin{align} \\frac{d}{dx}f(x) &amp;= \\frac{d}{dx}\\frac{1}{2}x^2 \\\\ &amp;= \\frac{1}{2} \\frac{d}{dx} x^2 \\\\ &amp;= \\frac{2}{2} x \\\\ &amp;= x \\end{align} \\] In general, the solution to any differential equation can be computed via integration \\(f(x) = \\int \\frac{d}{dx} f(x) dx\\). However, in practice, the differential equations we will want to solve are too complex to solve using either intuition or by explicitly evaluating integrals. 5.2 Euler’s method Euler’s method is a simple method to solve differential equations that can be applied in situations where a closed analytical solution cannot be easily obtained. Euler’s method says this: \\[f(x_2) \\approx f(x_1) + \\frac{d}{dx} f(x) \\Bigg\\rvert_{x=x_1} \\Delta x\\] In words, this says that the value of \\(f(x_2)\\) is approximately equal to \\(f(x_1)\\) plus how much it changed from \\(x_1\\) to \\(x_2\\). The how much it was likely to change bit is computed by taking the derivative evaluated at \\(x_1\\) and multiplying by the total change in \\(x\\), given by \\(\\Delta x = x_2 - x_1\\). Here’s how to implement Euler’s method in python: import numpy as np import matplotlib.pyplot as plt # define the range over which to approximate fx x = np.arange(0, 5, 0.01) # initialise fx to zeros fx = np.zeros(x.shape) # Euler&#39;s method requires we specify an initial value fx[0] = 1 for i in range(1, x.shape[0]): # df/dx = x dfxdx = x[i-1] # delta x dx = x[i] - x[i-1] # Euler&#39;s update fx[i] = fx[i-1] + dfxdx * dx # plot solution # It should look like 1/2 x^2 fig, ax, = plt.subplots(1, 1, squeeze=False) ax[0, 0].plot(x, fx) ax[0, 0].set_ylabel(&#39;f(x)&#39;) ax[0, 0].set_xlabel(&#39;x&#39;) plt.show() "],["simple-neuron-models.html", "6 Simple neuron models 6.1 Review of HH 6.2 Leaky integrate and fire 6.3 Quadratic integrate and fire 6.4 Izhikevich Neuron", " 6 Simple neuron models 6.1 Review of HH \\[ C \\frac{d}{dt}V = I - \\overline{g}_{K} n^4 (V - E_{K}) - \\overline{g}_{Na} m^3 h (V - E_{Na}) - \\overline{g}_{L} (V - E_{L})\\\\ \\frac{d}{dt}n = (n_{\\infty}(V) - n) / \\tau(V) \\\\ \\frac{d}{dt}m = (m_{\\infty}(V) - m) / \\tau(V) \\\\ \\frac{d}{dt}h = (h_{\\infty}(V) - h) / \\tau(V) \\\\ \\] HH model is an example of a conductance-based model One advantage of conductance-based models is that their parameters have well-defined biophysical meanings. However, this does not mean that measuring the true value of these parameters is easy. Rather, it is often difficult and noisy. Furthermore, it is difficult to ensure the model will behave like a real neuron outside the stimulation protocols used to measure model parameters. 6.2 Leaky integrate and fire \\[ \\frac{d}{dt} v(t) = b - v(t) \\\\ \\text{if } v(t) = 1, \\text{then } v(t) \\leftarrow 0 \\] Linearly adds up inputs until the membrane potential crosses a threshold, at which point a spike is triggered, and the membrane potential is reset. Spikes are all-or-none assumed to be identical in size and duration. Spikes aren’t actually part of the stimulation. They are just hand drawn in when threshold is crossed. There can be something of a refractory period if the reset potential is set to hyperpolarize the membrane potential. The amplitude of an input is linearly encoded into the frequency of spiking. This is called Class 1 excitability. import numpy as np import matplotlib.pyplot as plt T = 90 tau = 0.01 t = np.arange(0, T, tau) n = t.shape[0] v = np.zeros(n) b = np.concatenate(([0.0] * (t.shape[0] // 3), [1.001] * (t.shape[0] // 3), [0.0] * (t.shape[0] // 3))) for i in range(1, n): dvdt = b[i-1] - v[i-1] dt = t[i] - t[i-1] v[i] = v[i-1] + dvdt * dt if v[i] &gt; 1: v[i] = 0 fig, ax, = plt.subplots(1, 1, squeeze=False) ax[0, 0].plot(t, v) ax[0, 0].set_ylabel(&#39;v&#39;) ax[0, 0].set_xlabel(&#39;t&#39;) plt.show() 6.3 Quadratic integrate and fire \\[ \\frac{d}{dt} v(t) = b + v^2(t) \\\\ \\text{if } v(t) = v_{\\text{peak}}, \\text{then } v(t) \\leftarrow v_{\\text{reset}} \\] It is the simplest truly spiking model. That is, they are the simplest of models in which the upstroke of an action potential is generated by the differential equations themselves. It basically does all of the things the leaky integrate and fire neuron does but way better and with none of the weird issues stemming from hand drawing spikes. import numpy as np import matplotlib.pyplot as plt T = 90 tau = 0.01 t = np.arange(0, T, tau) n = t.shape[0] v = np.zeros(n) vreset = 0 vpeak = 1 v[0] = vreset b = np.concatenate(([0.0] * (n // 3), [0.02] * (n // 3), [0.0] * (n // 3))) for i in range(1, t.shape[0]): dvdt = b[i-1] + v[i-1]**2 dt = t[i] - t[i-1] v[i] = v[i-1] + dvdt * dt if v[i] &gt; vpeak: v[i] = vreset fig, ax, = plt.subplots(1, 1, squeeze=False) ax[0, 0].plot(t, v) ax[0, 0].set_ylabel(&#39;v&#39;) ax[0, 0].set_xlabel(&#39;t&#39;) plt.show() 6.4 Izhikevich Neuron \\[ \\begin{align} C \\frac{d}{dt} v(t) &amp;= k(v(t) - v_r) (v(t) - v_t) - u(t) + I(t) \\\\ \\frac{d}{dt} u(t) &amp;= a \\big(b(v(t) - v_r) - u(t)\\big) \\\\ \\text{ if} &amp;\\quad v(t) &gt; v_{\\text{peak}} \\\\ \\text{ then} &amp;\\quad v(t) \\leftarrow c, \\quad u(t) \\leftarrow u(t) + d \\end{align} \\] System of two coupled differential equations comprised of a fast voltage variable \\(v\\) and a slow recovery variable \\(u\\). All slow currents that modulate spike generation are combined in the variable \\(u\\) (e.g., the activation of the K\\(^+\\) current or inactivation of the Na\\(^+\\) current or their combination). Main advantage over quadratic integrate-and-fire is that it can capture a wide variety of spiking behaviour (e.g., bursting, chattering, etc.) as well as interesting subthreshold dynamics leading to spikes. The parameters \\(k\\) and \\(b\\) can be found when one knows the neuron’s rheobase and input resistance. The rheobase is roughly the minimal amplitude of continuous current that causes the membrane potential to reach the depolarization threshold. The sign of \\(b\\) determines whether \\(u\\) is an amplifying (\\(b &lt; 0\\)) or a resonant (\\(b &gt; 0\\)) variable. In the latter case, the neuron sags in response to hyperpolarized pulses of current, peaks in response to depolarized subthreshold pulses, and produces rebound (postinhibitory) responses. The parameter \\(a\\) is the recovery time constant. The spike cutoff value is \\(v_{\\text{peak}}\\), and the voltage reset value is \\(c\\). The parameter \\(d\\) describes the total amount of outward minus inward currents activated during the spike and affecting the after-spike behaviour. To examine the wide array of behaviours this model can generate without re-writing many lines of code, it will be helpful to use the following function: import numpy as np import matplotlib.pyplot as plt def izn(shape_params, spike_params, t_params, I): (a, b, c, d, k, C) = shape_params (vr, vt, vpeak) = spike_params (T, tau, t, n) = t_params u = np.zeros(n) v = np.zeros(n) v[0] = vr for i in range(1, n): dvdt = (k * (v[i - 1] - vr) * (v[i - 1] - vt) - u[i - 1] + I[i - 1]) / C dudt = a * (b * (v[i - 1] - vr) - u[i - 1]) dt = t[i] - t[i - 1] v[i] = v[i - 1] + dvdt * dt u[i] = u[i - 1] + dudt * dt if v[i] &gt; vpeak: v[i - 1] = vpeak v[i] = c u[i] = u[i] + d return v, u We can now control what type of neuron behaviour we get simply by setting the shape and spike parameters as follows. T = 1000 tau = 1 t = np.arange(0, T, tau) n = t.shape[0] I = np.zeros(n) I[n // 3:(2 * n // 3)] = 1 # Regular Spiking C = 100 vr = -60 vt = -40 vpeak = 35 a = 0.03 b = -2 c = -50 d = 100 k = 0.7 spike_params_regular_spiking = (vr, vt, vpeak) shape_params_regular_spiking = (a, b, c, d, k, C) # bursting C = 100 vr = -75 vt = -45 vpeak = 50 a = 0.01 b = 5 c = -56 d = 130 k = 1.2 spike_params_bursting = (vr, vt, vpeak) shape_params_bursting = (a, b, c, d, k, C) # chattering C = 50 vr = -60 vt = -40 vpeak = 25 a = 0.03 b = 1 c = -40 d = 150 k = 1.5 spike_params_chattering = (vr, vt, vpeak) shape_params_chattering = (a, b, c, d, k, C) t_params = (T, tau, t, n) v_regular_spiking, u_regular_spiking = izn(shape_params_regular_spiking, spike_params_regular_spiking, t_params, I * 100) v_bursting, u_bursting = izn(shape_params_bursting, spike_params_bursting, t_params, I * 500) v_chattering, u_chattering = izn(shape_params_chattering, spike_params_chattering, t_params, I * 200) fig, ax, = plt.subplots(3, 1, squeeze=False, figsize=(16, 10)) ax = ax.T ax[0, 0].plot(t, v_regular_spiking) ax[0, 1].plot(t, v_bursting) ax[0, 2].plot(t, v_chattering) ax[0, 0].set_title(&#39;Regular Spiking&#39;) ax[0, 0].set_xlabel(&#39;t&#39;) ax[0, 0].set_ylabel(&#39;v&#39;) ax[0, 1].set_title(&#39;Bursting&#39;) ax[0, 1].set_xlabel(&#39;t&#39;) ax[0, 1].set_ylabel(&#39;v&#39;) ax[0, 2].set_title(&#39;Chattering&#39;) ax[0, 2].set_xlabel(&#39;t&#39;) ax[0, 2].set_ylabel(&#39;v&#39;) plt.show() "],["hodgkin-huxley-neuron-model.html", "7 Hodgkin-Huxley neuron model 7.1 neuron_func_1 7.2 neuron_func_2 7.3 neuron_func_3 7.4 neuron_func_4 7.5 neuron_func_5 7.6 neuron_func_6 7.7 neuron_func_7", " 7 Hodgkin-Huxley neuron model import numpy as np import matplotlib.pyplot as plt 7.1 neuron_func_1 def neuron_func_1(): &#39;&#39;&#39; NOTE: dvdt = I - g * (v - E) - has stable eq at E - looks reasonable for sub threshold dynamics - nothing like an action potential &#39;&#39;&#39; tau = 0.01 T = 100 t = np.arange(0, T, tau) n = t.shape[0] I = np.zeros(n) v = np.zeros(n) vr = -65.0 E = -65.0 g = 1.0 C = 1.0 v[0] = vr I[n // 5:2 * n // 5] = 30.0 I[3 * n // 5:4 * n // 5] = -30.0 for i in range(1, n): delta_t = t[i] - t[i - 1] dvdt = (I[i - 1] - g * (v[i - 1] - E)) / C v[i] = v[i - 1] + dvdt * delta_t fig, ax = plt.subplots(3, 1, squeeze=False) ax[0, 0].plot(t, I, label=&#39;I&#39;) ax[1, 0].plot(t, g * (v - E), label=&#39;g * (v - E)&#39;) ax[2, 0].plot(t, v, label=&#39;v&#39;) [x.legend() for x in ax.flatten()] plt.tight_layout() plt.show() neuron_func_1() 7.2 neuron_func_2 def neuron_func_2(): &#39;&#39;&#39; NOTE: dvdt = I - g_a * (v - E_a) - g_b * (v - E_b) - has stable eq between E_a and E_b with balance determined by relative g&#39;s - still looks reasonable for sub threshold - still nothing like an action potential &#39;&#39;&#39; tau = 0.01 T = 100 t = np.arange(0, T, tau) n = t.shape[0] I = np.zeros(n) v = np.zeros(n) vr = -65.0 E_a = -65.0 E_b = 65.0 g_a = 1.0 g_b = 1.0 C = 1.0 v[0] = vr I[n // 5:2 * n // 5] = 30.0 I[3 * n // 5:4 * n // 5] = -30.0 for i in range(1, n): delta_t = t[i] - t[i - 1] dvdt = (I[i - 1] - g_a * (v[i - 1] - E_a) - g_b * (v[i - 1] - E_b)) / C v[i] = v[i - 1] + dvdt * delta_t fig, ax = plt.subplots(3, 1, squeeze=False) ax[0, 0].plot(t, I, label=&#39;I&#39;) ax[1, 0].plot(t, g_a * (v - E_a), label=&#39;g * (v - E)&#39;) ax[1, 0].plot(t, g_b * (v - E_b), label=&#39;g * (v - E)&#39;) ax[2, 0].plot(t, v, label=&#39;v&#39;) [x.legend() for x in ax.flatten()] plt.tight_layout() plt.show() neuron_func_2() 7.3 neuron_func_3 def neuron_func_3(): &#39;&#39;&#39; dvdt = I - g_a_max * a * (v - E_a) - g_b * (v - E_b) dadt = (a_inf - a) / a_tau - Increase due to I can be amplified - rise looks an action potential - no fall back to rest &#39;&#39;&#39; def g_func_inf(v): # sigmoid shape return 1 / (1 + 0.1 * np.exp(-0.1 * (v))) def g_func_tau(v): # bell shape mu = -10.0 sig = 50.0 return np.exp(-((v - mu) / sig)**2) tau = 0.01 T = 100 t = np.arange(0, T, tau) n = t.shape[0] I = np.zeros(n) v = np.zeros(n) g_a = np.zeros(n) g_b = np.zeros(n) vr = -65.0 E_a = -65.0 E_b = -65.0 g_a_max = 20.0 g_b = 1.0 C = 1.0 v[0] = vr g_a[0] = g_func_inf(vr) I[n // 5:2 * n // 5] = 30.0 I[3 * n // 5:4 * n // 5] = -30.0 for i in range(1, n): delta_t = t[i] - t[i - 1] dvdt = (I[i - 1] - g_a_max * g_a[i - 1] * (v[i - 1] - E_a) - g_b * (v[i - 1] - E_b)) / C dgdt = (g_func_inf(v[i - 1]) - g_a[i - 1]) / g_func_tau(v[i - 1]) v[i] = v[i - 1] + dvdt * delta_t g_a[i] = g_a[i - 1] + dgdt * delta_t fig, ax = plt.subplots(5, 1, squeeze=False) x = np.arange(-90, 90, 0.01) ax[0, 0].plot(x, g_func_inf(x), label=&#39;g_func_inf&#39;) ax[1, 0].plot(x, g_func_tau(x), label=&#39;g_func_tau&#39;) ax[2, 0].plot(t, I, label=&#39;I&#39;) ax[3, 0].plot(t, g_a_max * g_a * (v - E_a), label=&#39;g_max * g_a * (v - E_a)&#39;) ax[3, 0].plot(t, g_b * (v - E_b), label=&#39;g_b * (v - E_b)&#39;) ax[4, 0].plot(t, v, label=&#39;v&#39;) [x.legend() for x in ax.flatten()] plt.tight_layout() plt.show() neuron_func_3() 7.4 neuron_func_4 def neuron_func_4(): &#39;&#39;&#39; dvdt = I - g_a_max * a * (v - E_a) - g_b_max * b * (v - E_b) dadt = (a_inf - a) / a_tau dbdt = (b_inf - b) / b_tau - Increase due to I can be amplified - rise looks an action potential (due to a) - can get a fall back to E_b (due to b) - fall occurs too slowly to be a normal action potential &#39;&#39;&#39; def g_func_inf_a(v): # sigmoid shape return 1 / (1 + 0.1 * np.exp(-0.1 * (v))) def g_func_inf_b(v): # sigmoid shape return -1 / (1.1 + 0.1 * np.exp(-0.1 * (v))) + 1 def g_func_tau_a(v): # bell shape mu = -10.0 sig = 60.0 return (300 / sig) * np.exp(-((v - mu) / sig)**2) def g_func_tau_b(v): # bell shape mu = -10.0 sig = 50.0 return (300 / sig) * np.exp(-((v - mu) / sig)**2) tau = 0.01 T = 200 t = np.arange(0, T, tau) n = t.shape[0] I = np.zeros(n) v = np.zeros(n) g_a = np.zeros(n) g_b = np.zeros(n) vr = -65.0 E_a = 120.0 E_b = -70.0 g_a_max = 1.0 g_b_max = 2.0 C = 1.0 v[0] = vr g_a[0] = g_func_inf_a(vr) g_b[0] = g_func_inf_b(vr) I[n // 5:2 * n // 5] = 30.0 I[3 * n // 5:4 * n // 5] = -30.0 for i in range(1, n): delta_t = t[i] - t[i - 1] dvdt = (I[i - 1] - g_a_max * g_a[i - 1] * (v[i - 1] - E_a) - g_b_max * g_b[i - 1] * (v[i - 1] - E_b)) / C dgdt_a = (g_func_inf_a(v[i - 1]) - g_a[i - 1]) / g_func_tau_a(v[i - 1]) dgdt_b = (g_func_inf_b(v[i - 1]) - g_b[i - 1]) / g_func_tau_b(v[i - 1]) v[i] = v[i - 1] + dvdt * delta_t g_a[i] = g_a[i - 1] + dgdt_a * delta_t g_b[i] = g_b[i - 1] + dgdt_b * delta_t fig, ax = plt.subplots(5, 1, squeeze=False) x = np.arange(-90, 90, 0.01) ax[0, 0].plot(x, g_func_inf_a(x), label=&#39;g_func_inf_a&#39;) ax[0, 0].plot(x, g_func_inf_b(x), label=&#39;g_func_inf_b&#39;) ax[1, 0].plot(x, g_func_tau_a(x), label=&#39;g_func_tau_a&#39;) ax[1, 0].plot(x, g_func_tau_b(x), label=&#39;g_func_tau_b&#39;) ax[2, 0].plot(t, I, label=&#39;I&#39;) ax[3, 0].plot(t, g_a_max * g_a * (v - E_a), label=&#39;g_a_max * g_a * (v - E_a)&#39;) ax[3, 0].plot(t, g_b_max * g_b * (v - E_b), label=&#39;g_b_max * g_b * (v - E_b)&#39;) ax[4, 0].plot(t, v, label=&#39;v&#39;) [x.legend() for x in ax.flatten()] plt.tight_layout() plt.show() neuron_func_4() 7.5 neuron_func_5 def neuron_func_5(): &#39;&#39;&#39; dvdt = I - g_a_max * a_act**3 * a_inact * (v - E_a) - g_b * (v - E_b) dadt_act = (a_act_inf - a_act) / a_act_tau dadt_inact = (a_inact_inf - a_inact) / a_inact_tau - Increase due to I can be amplified - rise looks an action potential (due to a_act) - fall back to E_b (due to a_inact) - still not quite right... but why really? &#39;&#39;&#39; def g_func_inf_a_act(v): # sigmoid shape return 1 / (1 + 0.1 * np.exp(-0.5 * (v + 50.0))) def g_func_inf_a_inact(v): # sigmoid shape return -1 / (1.1 + 0.1 * np.exp(-0.1 * (v))) + 1 def g_func_tau_a_act(v): # bell shape mu = -10.0 sig = 60.0 return (300 / sig) * np.exp(-((v - mu) / sig)**2) def g_func_tau_a_inact(v): # bell shape mu = -10.0 sig = 50.0 return (300 / sig) * np.exp(-((v - mu) / sig)**2) tau = 0.01 T = 200 t = np.arange(0, T, tau) n = t.shape[0] I = np.zeros(n) v = np.zeros(n) g_a_act = np.zeros(n) g_a_inact = np.zeros(n) vr = -65.0 E_a = 120.0 E_b = -70.0 g_a_max = 2.0 g_b_max = 1.0 C = 1.0 v[0] = vr g_a_act[0] = g_func_inf_a_act(vr) g_a_inact[0] = g_func_inf_a_inact(vr) I[n // 5:2 * n // 5] = 30.0 I[3 * n // 5:4 * n // 5] = -30.0 for i in range(1, n): delta_t = t[i] - t[i - 1] dvdt = (I[i - 1] - g_a_max * g_a_act[i - 1]**3 * g_a_inact[i - 1] * (v[i - 1] - E_a) - g_b_max * (v[i - 1] - E_b)) / C dgdt_a_act = (g_func_inf_a_act(v[i - 1]) - g_a_act[i - 1]) / g_func_tau_a_act(v[i - 1]) dgdt_a_inact = (g_func_inf_a_inact(v[i - 1]) - g_a_inact[i - 1]) / g_func_tau_a_inact(v[i - 1]) v[i] = v[i - 1] + dvdt * delta_t g_a_act[i] = g_a_act[i - 1] + dgdt_a_act * delta_t g_a_inact[i] = g_a_inact[i - 1] + dgdt_a_inact * delta_t fig, ax = plt.subplots(5, 1, squeeze=False) x = np.arange(-90, 90, 0.01) ax[0, 0].plot(x, g_func_inf_a_act(x), label=&#39;g_func_inf_a_act&#39;) ax[0, 0].plot(x, g_func_inf_a_inact(x), label=&#39;g_func_inf_a_inact&#39;) ax[1, 0].plot(x, g_func_tau_a_act(x), label=&#39;g_func_tau_a_act&#39;) ax[1, 0].plot(x, g_func_tau_a_inact(x), label=&#39;g_func_tau_a_inact&#39;) ax[2, 0].plot(t, I, label=&#39;I&#39;) ax[3, 0].plot(t, g_a_max * g_a_act * (v - E_a), label=&#39;g_a_max * g_a_act * (v - E_a)&#39;) ax[3, 0].plot(t, g_a_max * g_a_inact * (v - E_a), label=&#39;g_a_max * g_a_inact * (v - E_a)&#39;) ax[4, 0].plot(t, v, label=&#39;v&#39;) [x.legend() for x in ax.flatten()] plt.tight_layout() plt.show() neuron_func_5() 7.6 neuron_func_6 def neuron_func_6(): &#39;&#39;&#39; dvdt = I - g_a_max * a_act**3 * a_inact * (v - E_a) - g_b_max * b * (v - E_b) dadt_act = (a_act_inf - a_act) / a_act_tau dadt_inact = (a_inact_inf - a_inact) / a_inact_tau dbdt = (b_inf - b) / b_tau - Increase due to I can be amplified - rise looks an action potential (due to a_act) - fall back to E_b (due to a_inact and due to b) - looking prettty good presumably &#39;&#39;&#39; def g_func_inf_a_act(v): # sigmoid shape return 1 / (1 + 0.1 * np.exp(-0.5 * (v + 50.0))) def g_func_inf_a_inact(v): # sigmoid shape return -1 / (1.1 + 0.1 * np.exp(-0.1 * (v))) + 1 def g_func_inf_b(v): # sigmoid shape return 1 / (1.1 + 0.1 * np.exp(-0.09 * (v))) def g_func_tau_a_act(v): # bell shape mu = -10.0 sig = 60.0 return (300 / sig) * np.exp(-((v - mu) / sig)**2) def g_func_tau_a_inact(v): # bell shape mu = -10.0 sig = 50.0 return (300 / sig) * np.exp(-((v - mu) / sig)**2) def g_func_tau_b(v): # bell shape mu = 5.0 sig = 50.0 return (300 / sig) * np.exp(-((v - mu) / sig)**2) tau = 0.01 T = 200 t = np.arange(0, T, tau) n = t.shape[0] I = np.zeros(n) v = np.zeros(n) g_a_act = np.zeros(n) g_a_inact = np.zeros(n) g_b = np.zeros(n) vr = -65.0 E_a = 120.0 E_b = -70.0 g_a_max = 2.0 g_b_max = 5.0 C = 1.0 v[0] = vr g_a_act[0] = g_func_inf_a_act(vr) g_a_inact[0] = g_func_inf_a_inact(vr) g_b[0] = g_func_inf_b(vr) I[n // 5:2 * n // 5] = 30.0 I[3 * n // 5:4 * n // 5] = -30.0 for i in range(1, n): delta_t = t[i] - t[i - 1] dvdt = (I[i - 1] - g_a_max * g_a_act[i - 1]**3 * g_a_inact[i - 1] * (v[i - 1] - E_a) - g_b_max * g_b[i - 1]**4 * (v[i - 1] - E_b)) / C dgdt_a_act = (g_func_inf_a_act(v[i - 1]) - g_a_act[i - 1]) / g_func_tau_a_act(v[i - 1]) dgdt_a_inact = (g_func_inf_a_inact(v[i - 1]) - g_a_inact[i - 1]) / g_func_tau_a_inact(v[i - 1]) dgdt_b = (g_func_inf_b(v[i - 1]) - g_b[i - 1]) / g_func_tau_b(v[i - 1]) v[i] = v[i - 1] + dvdt * delta_t g_a_act[i] = g_a_act[i - 1] + dgdt_a_act * delta_t g_a_inact[i] = g_a_inact[i - 1] + dgdt_a_inact * delta_t g_b[i] = g_b[i - 1] + dgdt_b * delta_t fig, ax = plt.subplots(5, 1, squeeze=False) x = np.arange(-90, 90, 0.01) ax[0, 0].plot(x, g_func_inf_a_act(x), label=&#39;g_func_inf_a_act&#39;) ax[0, 0].plot(x, g_func_inf_a_inact(x), label=&#39;g_func_inf_a_inact&#39;) ax[0, 0].plot(x, g_func_inf_b(x), label=&#39;g_func_inf_b&#39;) ax[1, 0].plot(x, g_func_tau_a_act(x), label=&#39;g_func_tau_a_act&#39;) ax[1, 0].plot(x, g_func_tau_a_inact(x), label=&#39;g_func_tau_a_inact&#39;) ax[1, 0].plot(x, g_func_tau_b(x), label=&#39;g_func_tau_b&#39;) ax[2, 0].plot(t, I, label=&#39;I&#39;) ax[3, 0].plot(t, g_a_max * g_a_act * (v - E_a), label=&#39;g_a_max * g_a_act * (v - E_a)&#39;) ax[3, 0].plot(t, g_a_max * g_a_inact * (v - E_a), label=&#39;g_a_max * g_a_inact * (v - E_a)&#39;) ax[3, 0].plot(t, g_b_max * g_b * (v - E_b), label=&#39;g_b_max * g_b * (v - E_b)&#39;) ax[4, 0].plot(t, v, label=&#39;v&#39;) [x.legend() for x in ax.flatten()] plt.tight_layout() plt.show() neuron_func_6() 7.7 neuron_func_7 def neuron_func_7(): &#39;&#39;&#39; NOTE: Full HH &#39;&#39;&#39; # NOTE: n gating variables def n_inf(v): return alpha_func_n(v) / (alpha_func_n(v) + beta_func_n(v)) def n_tau(v): return 1.0 / (alpha_func_n(v) + beta_func_n(v)) def alpha_func_n(v): return 0.01 * (10.0 - v) / (np.exp((10.0 - v) / 10.0) - 1.0) def beta_func_n(v): return 0.125 * np.exp(-v / 80.0) # NOTE: m gating variables def m_inf(v): return alpha_func_m(v) / (alpha_func_m(v) + beta_func_m(v)) def m_tau(v): return 1.0 / (alpha_func_m(v) + beta_func_m(v)) def alpha_func_m(v): return 0.1 * (25.0 - v) / (np.exp((25.0 - v) / 10.0) - 1.0) def beta_func_m(v): return 4.0 * np.exp(-v / 18.0) # NOTE: h gating variables def h_inf(v): return alpha_func_h(v) / (alpha_func_h(v) + beta_func_h(v)) def h_tau(v): return 1.0 / (alpha_func_h(v) + beta_func_h(v)) def alpha_func_h(v): return 0.07 * np.exp(-v / 20.0) def beta_func_h(v): return 1.0 / (np.exp((30.0 - v) / 10.0) + 1.0) # NOTE: simulation parameter etc tau = 0.01 T = 200 t = np.arange(0, T, tau) nn = t.shape[0] I = np.zeros(nn) v = np.zeros(nn) n = np.zeros(nn) m = np.zeros(nn) h = np.zeros(nn) vr = -65.0 v[0] = vr n[0] = n_inf(vr*0) m[0] = m_inf(vr*0) h[0] = h_inf(vr*0) g_k = 36.0 g_na = 120.0 g_leak = 0.30 E_k = -12 + vr E_na = 120 + vr E_leak = 10.6 + vr C = 1.0 I[nn // 5:2 * nn // 5] = 5.0 I[3 * nn // 5:4 * nn // 5] = -5.0 # NOTE: Euler&#39;s method simulation for i in range(1, nn): delta_t = t[i] - t[i - 1] I_k = g_k * (n[i - 1]**4) * (v[i - 1] - E_k) I_na = g_na * (m[i - 1]**3) * (h[i - 1]) * (v[i - 1] - E_na) I_leak = g_leak * (v[i - 1] - E_leak) dvdt = (I[i - 1] - (I_k + I_na + I_leak)) / C dndt = (n_inf(v[i - 1] - vr) - n[i - 1]) / n_tau(v[i - 1] - vr) dmdt = (m_inf(v[i - 1] - vr) - m[i - 1]) / m_tau(v[i - 1] - vr) dhdt = (h_inf(v[i - 1] - vr) - h[i - 1]) / h_tau(v[i - 1] - vr) v[i] = v[i - 1] + dvdt * delta_t n[i] = n[i - 1] + dndt * delta_t m[i] = m[i - 1] + dmdt * delta_t h[i] = h[i - 1] + dhdt * delta_t # NOTE: inspect gating functions fig, ax = plt.subplots(5, 1, squeeze=False) v_range = np.arange(-40, 100, 0.01) ax[0, 0].plot(v_range, n_inf(v_range), label=&#39;n: k activation&#39;) ax[0, 0].plot(v_range, m_inf(v_range), label=&#39;m: Na activation&#39;) ax[0, 0].plot(v_range, h_inf(v_range), label=&#39;h: Na inactivation&#39;) ax[1, 0].plot(v_range, n_tau(v_range), label=&#39;n: k activation&#39;) ax[1, 0].plot(v_range, m_tau(v_range), label=&#39;m: Na activation&#39;) ax[1, 0].plot(v_range, h_tau(v_range), label=&#39;h: Na inactivation&#39;) ax[2, 0].plot(t, I, label=&#39;I&#39;) ax[3, 0].plot(t, v, label=&#39;v&#39;) ax[4, 0].plot(t, n, label=&#39;n: k activation&#39;) ax[4, 0].plot(t, m, label=&#39;m: Na activation&#39;) ax[4, 0].plot(t, h, label=&#39;h: Na inactivation&#39;) [x.legend() for x in ax.flatten()] plt.tight_layout() plt.show() neuron_func_7() "],["neurotransmitter-release.html", "8 Neurotransmitter release 8.1 Synaptics responses 8.2 Spikes as a change in conductance 8.3 Coupling conductance to PSP 8.4 PSPs superimpose 8.5 PSP model in the Izhikevich neuron 8.6 Chaining neurons together", " 8 Neurotransmitter release 8.1 Synaptics responses Action potentials lead to neurotransmitter release which float across the synaptic cleft and bind to postsynaptic receptors on a receiving neuron, causing excitatory or inhibitory postsynaptic potentials (EPSP or IPSP) in the receiving neuron. The timecourse of the PSP on the receiving membrane potential is commonly modelled as follows. \\[ \\Delta V_{\\text{PSP}}(t) = A t e^{\\frac{-t}{t_{\\text{peak}}}} \\] \\(A\\) is amplitude parameter \\(t_{\\text{peak}}\\) is the time at which \\(\\Delta V_{\\text{PSP}}(t)\\) reaches its max value. In neuroscience, this function is often called the alpha function. import numpy as np import matplotlib.pyplot as plt tau = 0.1 T = 10 A = 1 t_peak = 2 t = np.arange(0, T, tau) n = t.shape[0] v_psp= A * t * np.exp(-t/t_peak) fig, ax = plt.subplots(1, 1, squeeze=False) ax[0,0].plot(t, v_psp) ax[0,0].set_xlabel(&#39;t&#39;) ax[0,0].set_ylabel(&#39;V&#39;) plt.show() 8.2 Spikes as a change in conductance This PSP timecourse can also be obtained by modelling the effect of an action potential on the ion conductivity of a receiving membrane potential as follows. \\[ \\frac{d}{dt}g(t) = \\big(-g(t) + A_{\\text{psp}} \\delta(t_{\\text{spike}}) \\big) / t_{\\text{psp}} \\] \\(g(t)\\) is the conductance of the receiving membrane patch. \\(A_{\\text{psp}}\\) is an amplitude parameter that determines how much each individual spike influences the conductance of the receiving neuron. \\(t_{\\text{psp}}\\) is a time constant that determines how quickly the influence of spike on conductance fades away. \\(t_{\\text{spike}}\\) is the time of the next spike and \\(\\delta()\\) is a delta function – it is one wherever \\(t = t_{\\text{spike}}\\) and zero everwhere else. g = np.zeros(n) spike = np.zeros(n) spike[20] = 1 for i in range(n): dgdt = (-g[i-1] + A * spike[i-1]) / t_peak dt = t[i] - t[i-1] g[i] = g[i-1] + dgdt * dt fig, ax = plt.subplots(1, 1, squeeze=False) ax[0,0].plot(t, g) ax[0,0].set_xlabel(&#39;t&#39;) ax[0,0].set_ylabel(&#39;g&#39;) plt.show() Notice that this model produces an instantaneous change in conductance. Therefore, this isn’t a particularly realistic model of conductance changes, but as we see next, it leads to PSPs that look pretty good. 8.3 Coupling conductance to PSP We couple the above model of conductance change to a simple model of membrane potential with the following simple differential equations. \\[ \\frac{d}{dt}v(t) = \\big( g(t) (v(t) - E) - g_{\\text{leak}} v(t) \\big) / C \\\\ \\frac{d}{dt}g(t) = \\big( -g(t) + A_{\\text{psp}} \\delta(t_{\\text{spike}}) \\big) / t_{\\text{psp}} \\] tau = 0.1 T = 20 A = 1 t_peak = 2 t = np.arange(0, T, tau) n = t.shape[0] v = np.zeros(n) g = np.zeros(n) spike = np.zeros(n) spike[20] = 1 E = -1 g_leak = 1 C=1 for i in range(n): dvdt = (g[i-1]*(v[i-1]-E) - g_leak * v[i-1]) / C dgdt = (-g[i-1] + A * spike[i-1]) / t_peak dt = t[i] - t[i-1] v[i] = v[i-1] + dvdt * dt g[i] = g[i-1] + dgdt * dt fig, ax = plt.subplots(2, 1, squeeze=False) ax[0,0].plot(t, v) ax[0,0].set_xlabel(&#39;t&#39;) ax[0,0].set_ylabel(&#39;v&#39;) ax[1,0].plot(t, g) ax[1,0].set_xlabel(&#39;t&#39;) ax[1,0].set_ylabel(&#39;g&#39;) plt.tight_layout() plt.show() 8.4 PSPs superimpose You can play around with the spike train frequency and number etc in the above to see how PSPs superimpose with multiple spikes. E.g., see below. tau = 0.1 T = 100 A = 1 t_peak = 2 t = np.arange(0, T, tau) n = t.shape[0] v = np.zeros(n) g = np.zeros(n) spike = np.zeros(n) spike[20::30] = 1 E = -10 g_leak = 1 C=1 for i in range(n): dvdt = (g[i-1]*(v[i-1]-E) - g_leak * v[i-1]) / C dgdt = (-g[i-1] + A * spike[i-1]) / t_peak dt = t[i] - t[i-1] v[i] = v[i-1] + dvdt * dt g[i] = g[i-1] + dgdt * dt fig, ax = plt.subplots(2, 1, squeeze=False) ax[0,0].plot(t, v) ax[0,0].set_xlabel(&#39;t&#39;) ax[0,0].set_ylabel(&#39;v&#39;) ax[1,0].plot(t, g) ax[1,0].set_xlabel(&#39;t&#39;) ax[1,0].set_ylabel(&#39;g&#39;) plt.tight_layout() plt.show() Note that you are not seeing action potentials from the above code. In fact, the above code doesn’t even have a mechanism to generate an action potential even if \\(v\\) became very depolarised. Rather, we are seeing PSPs. In practice, enough spikes will cuase many superimposing PSPs and that will cause the receiving cell to fire. We turn to that next. 8.5 PSP model in the Izhikevich neuron We can also easily embed this sort of modelling of PSPs into other neuron types. To build moderate to large networks, we will end up relying on Izhikevich neurons, so we look at that next. The differential equations for an Izhikevich neuron is given by the following: \\[ \\begin{align} C \\frac{d}{dt} v(t) &amp;= k(v(t) - v_r) (v(t) - v_t) - u(t) + I(t) \\\\ \\frac{d}{dt} u(t) &amp;= a \\big(b(v(t) - v_r) - u(t)\\big) \\\\ \\text{ if} &amp;\\quad v(t) &gt; v_{\\text{peak}} \\\\ \\text{ then} &amp;\\quad v(t) \\leftarrow c, \\quad u(t) \\leftarrow u(t) + d \\end{align} \\] Check back over previous lecture notes for a description of what each parameter does. The trickiest part is thinking about where to place \\(g(t)\\). The form of the Izhikevich neuron looks different enough from the leaky conductance model from above that it may isn’t immediately obvious what we should do. The key is to remember that since the \\(I\\) term is the input current, and a change in conductance effectively leads to changes in input currents, the \\(g\\) term is closely related to \\(I\\). Here, we take the simple approach of simply replacing \\(I\\) with the \\(g\\) term from an afferent neuron. An intuitive way of justifying this decision is to remember that we use \\(I(t)\\) to model any external current input, and this is precisely what a spike is from the perspective of the receiving cell. import numpy as np import matplotlib.pyplot as plt tau = 0.1 T = 100 t = np.arange(0, T, tau) n = t.shape[0] C = 50 vr = -80 vt = -25 vpeak = 40 k = 1 a = 0.01 b = -20 c = -55 d = 150 spike = np.zeros(n) spike[200] = 100 g = np.zeros(n) psp_amp = 100 psp_decay = 10 v = np.zeros(n) u = np.zeros(n) v[0] = vr for i in range(1, n): # model the current input from some afferent neuron dgdt = (-g[i - 1] + psp_amp * spike[i - 1]) / psp_decay dvdt = (k * (v[i - 1] - vr) * (v[i - 1] - vt) - u[i - 1] + g[i-1]) / C dudt = a * (b * (v[i - 1] - vr) - u[i - 1]) dt = t[i] - t[i - 1] v[i] = v[i - 1] + dvdt * dt u[i] = u[i - 1] + dudt * dt g[i] = g[i - 1] + dgdt * dt if v[i] &gt;= vpeak: v[i - 1] = vpeak v[i] = c u[i] = u[i] + d fig, ax = plt.subplots(2, 1, squeeze=False) ax[0, 0].plot(t, v) ax[1, 0].plot(t, g) plt.tight_layout() plt.show() 8.6 Chaining neurons together We now build a simple spiking neural network that chains three excitatory neurons together in a straightforward manner: \\(A \\rightarrow B \\rightarrow C\\) The basic logic that we use here is pretty simple. There will be an external input (square pulse thing like we have used lots so far in this unit) that feeds into neuron \\(A\\), and then the neurotransmitter release assocaited with each neurons action potentials will induce EPSPs in the neuron that they project to. import numpy as np import matplotlib.pyplot as plt tau = 0.1 T = 100 t = np.arange(0, T, tau) n = t.shape[0] C = 50 vr = -80 vt = -25 vpeak = 40 k = 1 a = 0.01 b = -20 c = -55 d = 150 psp_amp = 1e5 psp_decay = 10 g = np.zeros(n) spike = np.zeros(n) spike[200:800:20] = 1 v1 = np.zeros(n) u1 = np.zeros(n) g1 = np.zeros(n) spike1 = np.zeros(n) v1[0] = vr v2 = np.zeros(n) u2 = np.zeros(n) g2 = np.zeros(n) spike2 = np.zeros(n) v2[0] = vr v3 = np.zeros(n) u3 = np.zeros(n) g3 = np.zeros(n) spike3 = np.zeros(n) v3[0] = vr w_01 = 1.0 w_12 = 0.5 w_23 = 0.1 for i in range(1, n): dt = t[i] - t[i - 1] # external input dgdt = (-g[i - 1] + psp_amp * spike[i - 1]) / psp_decay g[i] = g[i - 1] + dgdt * dt # neuron 1 dvdt1 = (k * (v1[i - 1] - vr) * (v1[i - 1] - vt) - u1[i - 1] + w_01 * g[i-1]) / C dudt1 = a * (b * (v1[i - 1] - vr) - u1[i - 1]) dgdt1 = (-g1[i - 1] + psp_amp * spike1[i - 1]) / psp_decay v1[i] = v1[i - 1] + dvdt1 * dt u1[i] = u1[i - 1] + dudt1 * dt g1[i] = g1[i - 1] + dgdt1 * dt if v1[i] &gt;= vpeak: v1[i - 1] = vpeak v1[i] = c u1[i] = u1[i] + d spike1[i] = 1 # neuron 2 dvdt2 = (k * (v2[i - 1] - vr) * (v2[i - 1] - vt) - u2[i - 1] + w_12 * g1[i-1]) / C dudt2 = a * (b * (v2[i - 1] - vr) - u2[i - 1]) dgdt2 = (-g2[i - 1] + psp_amp * spike2[i - 1]) / psp_decay v2[i] = v2[i - 1] + dvdt2 * dt u2[i] = u2[i - 1] + dudt2 * dt g2[i] = g2[i - 1] + dgdt2 * dt if v2[i] &gt;= vpeak: v2[i - 1] = vpeak v2[i] = c u2[i] = u2[i] + d spike2[i] = 1 # neuron 3 dvdt3 = (k * (v3[i - 1] - vr) * (v3[i - 1] - vt) - u3[i - 1] + w_23 * g2[i-1]) / C dudt3 = a * (b * (v3[i - 1] - vr) - u3[i - 1]) dgdt3 = (-g3[i - 1] + psp_amp * spike3[i - 1]) / psp_decay v3[i] = v3[i - 1] + dvdt3 * dt u3[i] = u3[i - 1] + dudt3 * dt g3[i] = g3[i - 1] + dgdt3 * dt if v3[i] &gt;= vpeak: v3[i - 1] = vpeak v3[i] = c u3[i] = u3[i] + d spike3[i] = 1 fig, ax = plt.subplots(2, 3, squeeze=False, figsize=(10, 5)) ax[0, 0].plot(t, g) ax[1, 0].plot(t, v1) ax[0, 1].plot(t, g1) ax[1, 1].plot(t, v2) ax[0, 2].plot(t, g2) ax[1, 2].plot(t, v3) ax[0, 0].set_title(&#39;Neuron A&#39;) ax[0, 1].set_title(&#39;Neuron B&#39;) ax[0, 2].set_title(&#39;Neuron C&#39;) ax[0, 0].set_xlabel(&#39;time&#39;) ax[0, 1].set_xlabel(&#39;time&#39;) ax[0, 2].set_xlabel(&#39;time&#39;) ax[1, 0].set_ylabel(&#39;Membrane Potential&#39;) ax[1, 1].set_ylabel(&#39;Membrane Potential&#39;) ax[1, 2].set_ylabel(&#39;Membrane Potential&#39;) ax[0, 0].set_ylabel(&#39;Neurotransmitter Output&#39;) ax[0, 1].set_ylabel(&#39;Neurotransmitter Output&#39;) ax[0, 2].set_ylabel(&#39;Neurotransmitter Output&#39;) plt.tight_layout() plt.show() "],["synpatic-plasticity.html", "9 Synpatic Plasticity 9.1 Python helper functions 9.2 Hebbian Learning 1 9.3 Hebbian Learning 2 9.4 Hebbian Learning 3 9.5 Hebbian STDP", " 9 Synpatic Plasticity Synaptic plasticity refers to changes in the connection weights between neurons. Long-term potentiation (LTP) is synaptic strengthening. Long-term depression (LTD) is synaptic strengthening. Synaptic plasticity comes in many different forms and operates by different computational principles in different brain regions and between different cell types. In this lecture, we will explore a few different types of Hebbian synaptic plasticity. 9.1 Python helper functions We will simulate a two neuron network in which neuron 1 projects to neuron 2, the first neuron is driven by an external input, and the connection weight between neuron 1 and neuron 2 is plastic. Since we will vary the rules that govern plasticity between neuron 1 and neuron 2, we will first write a bit of python code that will help us avoid rewriting the same lines of code over and over. import numpy as np import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec def init_arrays(): v1 = np.zeros(n) u1 = np.zeros(n) g1 = np.zeros(n) spike1 = np.zeros(n) v1[0] = vr v2 = np.zeros(n) u2 = np.zeros(n) g2 = np.zeros(n) spike2 = np.zeros(n) v2[0] = vr w_01 = 0.4 * np.ones(n_trials) w_12 = 0.4 * np.ones(n_trials) g_record = np.zeros((n_trials, n)) v1_record = np.zeros((n_trials, n)) g1_record = np.zeros((n_trials, n)) v2_record = np.zeros((n_trials, n)) g2_record = np.zeros((n_trials, n)) return { &#39;v1&#39;: v1, &#39;u1&#39;: u1, &#39;g1&#39;: g1, &#39;spike1&#39;: spike1, &#39;v2&#39;: v2, &#39;u2&#39;: u2, &#39;g2&#39;: g2, &#39;spike2&#39;: spike2, &#39;w_01&#39;: w_01, &#39;w_12&#39;: w_12, &#39;g_record&#39;: g_record, &#39;v1_record&#39;: v1_record, &#39;g1_record&#39;: g1_record, &#39;v2_record&#39;: v2_record, &#39;g2_record&#39;: g2_record } def plot_results(): fig = plt.figure(figsize=(8, 8)) gs = gridspec.GridSpec(4, 2) ax00 = fig.add_subplot(gs[0, 0]) ax10 = fig.add_subplot(gs[1, 0]) ax20 = fig.add_subplot(gs[2, 0]) ax01 = fig.add_subplot(gs[0, 1]) ax11 = fig.add_subplot(gs[1, 1]) ax21 = fig.add_subplot(gs[2, 1]) ax3 = fig.add_subplot(gs[3, :]) ax1 = ax00 ax2 = ax00.twinx() ax2.plot(t, g_record[0], &#39;C0&#39;, label=&#39;external g&#39;) ax2.legend(loc=&#39;lower right&#39;) ax1.set_title(&#39;Trial 1&#39;) ax1.set_xlabel(&#39;time (t)&#39;) ax1 = ax10 ax2 = ax10.twinx() ax1.plot(t, v1_record[0], &#39;C0&#39;, label=&#39;v1&#39;) ax2.plot(t, g1_record[0], &#39;C1&#39;, label=&#39;g1&#39;) ax1.legend(loc=&#39;upper right&#39;) ax2.legend(loc=&#39;lower right&#39;) ax1.set_xlabel(&#39;time (t)&#39;) ax1 = ax20 ax2 = ax20.twinx() ax1.plot(t, v2_record[0], &#39;C0&#39;, label=&#39;v2&#39;) ax2.plot(t, g2_record[0], &#39;C1&#39;, label=&#39;g2&#39;) ax1.legend(loc=&#39;upper right&#39;) ax2.legend(loc=&#39;lower right&#39;) ax1.set_xlabel(&#39;time (t)&#39;) ax1 = ax01 ax2 = ax01.twinx() ax2.plot(t, g_record[0], &#39;C0&#39;, label=&#39;external g&#39;) ax2.legend(loc=&#39;lower right&#39;) ax1.set_title(&#39;Trial n&#39;) ax1.set_xlabel(&#39;time (t)&#39;) ax1 = ax11 ax2 = ax11.twinx() ax1.plot(t, v1_record[-2], &#39;C0&#39;, label=&#39;v1&#39;) ax2.plot(t, g1_record[-2], &#39;C1&#39;, label=&#39;g1&#39;) ax1.legend(loc=&#39;upper right&#39;) ax2.legend(loc=&#39;lower right&#39;) ax1.set_xlabel(&#39;time (t)&#39;) ax1 = ax21 ax2 = ax21.twinx() ax1.plot(t, v2_record[-2], &#39;C0&#39;, label=&#39;v2&#39;) ax2.plot(t, g2_record[-2], &#39;C1&#39;, label=&#39;g2&#39;) ax1.legend(loc=&#39;upper right&#39;) ax2.legend(loc=&#39;lower right&#39;) ax1.set_xlabel(&#39;time (t)&#39;) ax3.plot(np.arange(0, n_trials, 1), w_12) ax3.set_xlabel(&#39;Trial&#39;) ax3.set_ylabel(&#39;Synaptic Weight (w)&#39;) plt.tight_layout() plt.show() def simulate_network(update_weight_func): for j in range(n_trials - 1): trl = j for i in range(1, n): dt = t[i] - t[i - 1] # external input dgdt = (-g[i - 1] + psp_amp * spike[i - 1]) / psp_decay g[i] = g[i - 1] + dgdt * dt # neuron 1 dvdt1 = (k * (v1[i - 1] - vr) * (v1[i - 1] - vt) - u1[i - 1] + w_01[trl] * g[i - 1]) / C dudt1 = a * (b * (v1[i - 1] - vr) - u1[i - 1]) dgdt1 = (-g1[i - 1] + psp_amp * spike1[i - 1]) / psp_decay v1[i] = v1[i - 1] + dvdt1 * dt u1[i] = u1[i - 1] + dudt1 * dt g1[i] = g1[i - 1] + dgdt1 * dt if v1[i] &gt;= vpeak: v1[i - 1] = vpeak v1[i] = c u1[i] = u1[i] + d spike1[i] = 1 # neuron 2 dvdt2 = (k * (v2[i - 1] - vr) * (v2[i - 1] - vt) - u2[i - 1] + w_12[trl] * g1[i - 1]) / C dudt2 = a * (b * (v2[i - 1] - vr) - u2[i - 1]) dgdt2 = (-g2[i - 1] + psp_amp * spike2[i - 1]) / psp_decay v2[i] = v2[i - 1] + dvdt2 * dt u2[i] = u2[i - 1] + dudt2 * dt g2[i] = g2[i - 1] + dgdt2 * dt if v2[i] &gt;= vpeak: v2[i - 1] = vpeak v2[i] = c u2[i] = u2[i] + d spike2[i] = 1 # update synaptic weights delta_w = update_weight_func() w_12[trl + 1] = w_12[trl] + delta_w # store trial info g_record[trl, :] = g v1_record[trl, :] = v1 g1_record[trl, :] = g1 v2_record[trl, :] = v2 g2_record[trl, :] = g2 plot_results() n_trials = 10 trl = 0 tau = 0.1 T = 100 t = np.arange(0, T, tau) n = t.shape[0] C = 50 vr = -80 vt = -25 vpeak = 40 k = 1 a = 0.01 b = -20 c = -55 d = 150 psp_amp = 1e5 psp_decay = 10 g = np.zeros(n) spike = np.zeros(n) spike[200:800:20] = 1 9.2 Hebbian Learning 1 \\[ w_{ij}(n+1) = w_{ij}(n) + \\alpha A_i(n) A_j(n) \\] Neurons that fire together wire together. Simple equation that can only generate LTP. It cannot generate LTD. Weakness is that it predicts all plastic synapses ultimately grow to infinity. This is because \\(A_i(n)&gt;0\\) and \\(A_j(n)&gt;0\\). The following Python code implements this learning rule. alpha = 3e-14 def update_weight_1(): pre = g1.sum() post = g2.sum() delta_w = alpha * pre * post return delta_w We next initialise the needed arrays and simulate the network using the above learning rule. array_dict = init_arrays() v1 = array_dict[&#39;v1&#39;] u1 = array_dict[&#39;u1&#39;] g1 = array_dict[&#39;g1&#39;] spike1 = array_dict[&#39;spike1&#39;] v2 = array_dict[&#39;v2&#39;] u2 = array_dict[&#39;u2&#39;] g2 = array_dict[&#39;g2&#39;] spike2 = array_dict[&#39;spike2&#39;] w_01 = array_dict[&#39;w_01&#39;] w_12 = array_dict[&#39;w_12&#39;] g_record = array_dict[&#39;g_record&#39;] v1_record = array_dict[&#39;v1_record&#39;] g1_record = array_dict[&#39;g1_record&#39;] v2_record = array_dict[&#39;v2_record&#39;] g2_record = array_dict[&#39;g2_record&#39;] update_weight_func = update_weight_1 alpha = 3e-14 simulate_network(update_weight_func) 9.3 Hebbian Learning 2 \\[ \\begin{align} w_{ij}(n+1) &amp;= w_{ij}(n) \\\\ &amp; + \\alpha H[A_j(n) - \\theta] A_i(n)\\\\ &amp; - \\beta H[\\theta - A_j(n)] A_i(n) \\end{align} \\] Simple augmentation of the Hebbian learning rule above. Assumes an activity threshold \\(\\theta\\) that can cause either LTP or LTD to occur. \\(H[z(x)]\\) is the Heaviside function and is \\(0\\) if \\(z(x)&lt;0\\) and is \\(z(x)\\) otherwise. Thus, this equation says that if the activation in the postsynapctic neuron is strong enough, then LTP will occur. If there is only weak activation (i.e., less than \\(\\theta\\)) then LTD will occur. This is an improvement because it is still quite simple and has both LTP and LTD. However, it can still grow to infinity or – even more strangely – negative infinity. The following Python code implements this learning rule. alpha = 3e-14 beta = 3e-14 theta = 2e6 # make this small for LTP or large for LTD def update_weight_2(): pre = g1.sum() post_ltp = np.clip(g2.sum() - theta, 0, None) post_ltd = np.clip(theta - g2.sum(), 0, None) delta_w = alpha * pre * post_ltp - beta * pre * post_ltd return delta_w We next initialise the needed arrays and simulate the network using the above learning rule. array_dict = init_arrays() v1 = array_dict[&#39;v1&#39;] u1 = array_dict[&#39;u1&#39;] g1 = array_dict[&#39;g1&#39;] spike1 = array_dict[&#39;spike1&#39;] v2 = array_dict[&#39;v2&#39;] u2 = array_dict[&#39;u2&#39;] g2 = array_dict[&#39;g2&#39;] spike2 = array_dict[&#39;spike2&#39;] w_01 = array_dict[&#39;w_01&#39;] w_12 = array_dict[&#39;w_12&#39;] g_record = array_dict[&#39;g_record&#39;] v1_record = array_dict[&#39;v1_record&#39;] g1_record = array_dict[&#39;g1_record&#39;] v2_record = array_dict[&#39;v2_record&#39;] g2_record = array_dict[&#39;g2_record&#39;] update_weight_func = update_weight_2 simulate_network(update_weight_func) 9.4 Hebbian Learning 3 \\[ \\begin{align} w_{ij}(n+1) &amp;= w_{ij}(n) \\\\ &amp;+ \\alpha H[A_j(n)-\\theta] [A_i(n)] [1 - w_{ij}(n)] \\\\ &amp;- \\beta H[\\theta-A_j(n)] [A_i(n)] [w_{ij}(n)] \\end{align} \\] Add a rate-limiting terms to prevent growth to \\(\\pm\\) infinity The rest is the same as above. The following Python code implements this learning rule. alpha = 3e-14 beta = 3e-14 theta = 2e6 def update_weight_3(): pre = g1.sum() post_ltp = np.clip(g2.sum() - theta, 0, None) * (1 - w_12[trl]) post_ltd = np.clip(theta - g2.sum(), 0, None) * w_12[trl] delta_w = alpha * pre * post_ltp - beta * pre * post_ltd return delta_w We next initialise the needed arrays and simulate the network using the above learning rule. array_dict = init_arrays() v1 = array_dict[&#39;v1&#39;] u1 = array_dict[&#39;u1&#39;] g1 = array_dict[&#39;g1&#39;] spike1 = array_dict[&#39;spike1&#39;] v2 = array_dict[&#39;v2&#39;] u2 = array_dict[&#39;u2&#39;] g2 = array_dict[&#39;g2&#39;] spike2 = array_dict[&#39;spike2&#39;] w_01 = array_dict[&#39;w_01&#39;] w_12 = array_dict[&#39;w_12&#39;] g_record = array_dict[&#39;g_record&#39;] v1_record = array_dict[&#39;v1_record&#39;] g1_record = array_dict[&#39;g1_record&#39;] v2_record = array_dict[&#39;v2_record&#39;] g2_record = array_dict[&#39;g2_record&#39;] update_weight_func = update_weight_3 simulate_network(update_weight_func) ## &lt;string&gt;:2: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`. 9.5 Hebbian STDP \\[ \\begin{align} w_{ij}(n+1) &amp;= w_{ij}(n) \\\\ &amp;+ \\alpha H[A_j(n)-\\theta] [A_i(n)] [1 - w_{ij}(n)] \\Delta \\\\ &amp;- \\beta H[\\theta-A_j(n)] [A_i(n)] [w_{ij}(n)] \\end{align} \\] \\[ \\begin{align} \\Delta = \\begin{cases} e^{-(T_{\\text{post}} - T_{\\text{pre}}) / \\tau_+} &amp;\\text{ if } T_{\\text{post}} &gt; T_{\\text{pre}} \\\\ -e^{ (T_{\\text{post}} - T_{\\text{pre}}) / \\tau_-} &amp;\\text{ if } T_{\\text{post}} &lt; T_{\\text{pre}} \\\\ \\end{cases} \\end{align} \\] Synaptic plasticity depends on the timing of spikes as well as the overall magnitude of neural activations between pre and postsynatpic neurons. LTP is caused if the presynaptic neuron fires before the postsynaptic neuron. LTD is caused in the inverse situation. The magnitude of LTP / LTD falls off exponentially as the time between pre and postsynaptic spikes gets larger. The following Python code implements this learning rule. alpha = 3e-14 beta = 3e-14 theta = 2e6 tau_pos = 10 tau_neg = 10 def update_weight_stdp(): delta_stdp = 0 n_pre_spikes = spike1.sum().astype(int) n_post_spikes = spike2.sum().astype(int) pre_spike_times = t[spike1==1] post_spike_times = t[spike2==1] for i in range(n_pre_spikes): for j in range(n_post_spikes): T_pre = pre_spike_times[i] T_post = post_spike_times[j] delta_T = T_post - T_pre if delta_T &gt; 0: delta_stdp += np.exp(-(delta_T)/tau_pos) else: delta_stdp += np.exp((delta_T)/tau_neg) pre = g1.sum() post_above_thresh = np.clip(g2.sum() - theta, 0, None) * (1 - w_12[trl]) * delta_stdp post_below_thresh = np.clip(theta - g2.sum(), 0, None) * w_12[trl] delta_w = alpha * pre * post_above_thresh - beta * pre * post_below_thresh return delta_w We next initialise the needed arrays and simulate the network using the above learning rule. array_dict = init_arrays() v1 = array_dict[&#39;v1&#39;] u1 = array_dict[&#39;u1&#39;] g1 = array_dict[&#39;g1&#39;] spike1 = array_dict[&#39;spike1&#39;] v2 = array_dict[&#39;v2&#39;] u2 = array_dict[&#39;u2&#39;] g2 = array_dict[&#39;g2&#39;] spike2 = array_dict[&#39;spike2&#39;] w_01 = array_dict[&#39;w_01&#39;] w_12 = array_dict[&#39;w_12&#39;] g_record = array_dict[&#39;g_record&#39;] v1_record = array_dict[&#39;v1_record&#39;] g1_record = array_dict[&#39;g1_record&#39;] v2_record = array_dict[&#39;v2_record&#39;] g2_record = array_dict[&#39;g2_record&#39;] update_weight_func = update_weight_stdp simulate_network(update_weight_func) "],["reinforcement-learning.html", "10 Reinforcement Learning", " 10 Reinforcement Learning "],["reinforcement-learning-rule.html", "11 Reinforcement learning rule", " 11 Reinforcement learning rule Strengthen synapses responsible for behaviour that lead to a better-than-expected outcome. Weaken synapses responsible for behaviour that lead to a worse-than-expected outcome. Do not change synapses at all if the outcome was fully expected. Whether or not an outcome was expected or not is captured by the prediction error which is usually denoted by \\(\\delta\\). A simple RL learning rule can be obtained by modifying the simple Hebbian rule as follows: \\[ \\begin{align} w_{ij}(n+1) &amp;= w_{ij}(n) + \\alpha A_j(n) A_i(n) \\delta(n) \\\\ \\end{align} \\] Outcomes are usually put in terms of reward which is denoted below by \\(r\\). To compute a predicted error, we simply need to know what reward we obtained – \\(r_{\\text{obtained}}\\) – and what reward we predicted \\(r_{\\text{predicted}}\\). The prediction error is just the difference between these two things. \\[ \\begin{align} \\delta = r_{\\text{obtained}} - r_{\\text{predicted}} \\end{align} \\] \\(r_{\\text{obtained}}\\) is specific to the agents behaviour and the environment it is acting within. We will play around with the various ways this can be structured a bit later. \\[ \\begin{align} r_{\\text{obtained}} = \\text{to be determined by the experiment} \\end{align} \\] \\(r_{\\text{predicted}}\\) is something the agent learns – i.e., it is the agent’s estimate of the reward that will be obtained. A good estimate of \\(r_{\\text{obtained}}\\) is the sample mean of all previously obtained rewards. \\[ \\begin{align} r_{\\text{predicted}} = \\frac{1}{n} \\sum_1^n r_{\\text{obtained}} \\end{align} \\] The sample mean can be computed iteratively with the following: \\[ \\begin{align} r_{\\text{predicted}}(t) = r_{\\text{predicted}}(t-1) + \\gamma \\delta \\end{align} \\] "],["example-1.html", "12 Example 1", " 12 Example 1 We will simulate a two neuron network in which neuron 1 projects to neuron 2, the first neuron is driven by an external input, and the connection weight between neuron 1 and neuron 2 is subject to reinforcement learning. We will use the following python code based on the previous lecture. The key additions to the code from previous lecture are mostly found in the update_weight_rl() function. import numpy as np import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec def init_arrays(): r_predicted = np.zeros(n_trials) r_obtained = np.zeros(n_trials) delta = np.zeros(n_trials) r_obtained[:n_trials // 4] = 1 r_obtained[(2 * n_trials // 4):(3 * n_trials // 4)] = 1 v1 = np.zeros(n) u1 = np.zeros(n) g1 = np.zeros(n) spike1 = np.zeros(n) v1[0] = vr v2 = np.zeros(n) u2 = np.zeros(n) g2 = np.zeros(n) spike2 = np.zeros(n) v2[0] = vr w_01 = 0.4 * np.ones(n_trials) w_12 = 0.4 * np.ones(n_trials) g_record = np.zeros((n_trials, n)) v1_record = np.zeros((n_trials, n)) g1_record = np.zeros((n_trials, n)) v2_record = np.zeros((n_trials, n)) g2_record = np.zeros((n_trials, n)) return { &#39;r_predicted&#39;: r_predicted, &#39;r_obtained&#39;: r_obtained, &#39;delta&#39;: delta, &#39;v1&#39;: v1, &#39;u1&#39;: u1, &#39;g1&#39;: g1, &#39;spike1&#39;: spike1, &#39;v2&#39;: v2, &#39;u2&#39;: u2, &#39;g2&#39;: g2, &#39;spike2&#39;: spike2, &#39;w_01&#39;: w_01, &#39;w_12&#39;: w_12, &#39;g_record&#39;: g_record, &#39;v1_record&#39;: v1_record, &#39;g1_record&#39;: g1_record, &#39;v2_record&#39;: v2_record, &#39;g2_record&#39;: g2_record } def plot_results(): fig = plt.figure(figsize=(8, 8)) gs = gridspec.GridSpec(5, 2) ax00 = fig.add_subplot(gs[0, 0]) ax10 = fig.add_subplot(gs[1, 0]) ax20 = fig.add_subplot(gs[2, 0]) ax01 = fig.add_subplot(gs[0, 1]) ax11 = fig.add_subplot(gs[1, 1]) ax21 = fig.add_subplot(gs[2, 1]) ax3 = fig.add_subplot(gs[3, :]) ax4 = fig.add_subplot(gs[4, :]) ax1 = ax00 ax2 = ax00.twinx() ax2.plot(t, g_record[0], &#39;C0&#39;, label=&#39;external g&#39;) ax2.legend(loc=&#39;lower right&#39;) ax1.set_title(&#39;Trial 1&#39;) ax1.set_xlabel(&#39;time (t)&#39;) ax1 = ax10 ax2 = ax10.twinx() ax1.plot(t, v1_record[0], &#39;C0&#39;, label=&#39;v1&#39;) ax2.plot(t, g1_record[0], &#39;C1&#39;, label=&#39;g1&#39;) ax1.legend(loc=&#39;upper right&#39;) ax2.legend(loc=&#39;lower right&#39;) ax1.set_xlabel(&#39;time (t)&#39;) ax1 = ax20 ax2 = ax20.twinx() ax1.plot(t, v2_record[0], &#39;C0&#39;, label=&#39;v2&#39;) ax2.plot(t, g2_record[0], &#39;C1&#39;, label=&#39;g2&#39;) ax1.legend(loc=&#39;upper right&#39;) ax2.legend(loc=&#39;lower right&#39;) ax1.set_xlabel(&#39;time (t)&#39;) ax1 = ax01 ax2 = ax01.twinx() ax2.plot(t, g_record[0], &#39;C0&#39;, label=&#39;external g&#39;) ax2.legend(loc=&#39;lower right&#39;) ax1.set_title(&#39;Trial n&#39;) ax1.set_xlabel(&#39;time (t)&#39;) ax1 = ax11 ax2 = ax11.twinx() ax1.plot(t, v1_record[-2], &#39;C0&#39;, label=&#39;v1&#39;) ax2.plot(t, g1_record[-2], &#39;C1&#39;, label=&#39;g1&#39;) ax1.legend(loc=&#39;upper right&#39;) ax2.legend(loc=&#39;lower right&#39;) ax1.set_xlabel(&#39;time (t)&#39;) ax1 = ax21 ax2 = ax21.twinx() ax1.plot(t, v2_record[-2], &#39;C0&#39;, label=&#39;v2&#39;) ax2.plot(t, g2_record[-2], &#39;C1&#39;, label=&#39;g2&#39;) ax1.legend(loc=&#39;upper right&#39;) ax2.legend(loc=&#39;lower right&#39;) ax1.set_xlabel(&#39;time (t)&#39;) ax3.plot(np.arange(0, n_trials, 1), r_obtained, label=&#39;r_obtained&#39;) ax3.plot(np.arange(0, n_trials, 1), r_predicted, label=&#39;r_predicted&#39;) ax3.plot(np.arange(0, n_trials, 1), delta, label=&#39;delta&#39;) ax3.set_xlabel(&#39;Trial&#39;) ax3.set_ylabel(&#39;&#39;) ax3.legend() ax4.plot(np.arange(0, n_trials, 1), w_12) ax4.set_xlabel(&#39;Trial&#39;) ax4.set_ylabel(&#39;Synaptic Weight (w)&#39;) plt.tight_layout() plt.show() def simulate_network(update_weight_func): global trl, r_obtained, r_predicted for j in range(n_trials - 1): trl = j for i in range(1, n): dt = t[i] - t[i - 1] # external input dgdt = (-g[i - 1] + psp_amp * spike[i - 1]) / psp_decay g[i] = g[i - 1] + dgdt * dt # neuron 1 dvdt1 = (k * (v1[i - 1] - vr) * (v1[i - 1] - vt) - u1[i - 1] + w_01[trl] * g[i - 1]) / C dudt1 = a * (b * (v1[i - 1] - vr) - u1[i - 1]) dgdt1 = (-g1[i - 1] + psp_amp * spike1[i - 1]) / psp_decay v1[i] = v1[i - 1] + dvdt1 * dt u1[i] = u1[i - 1] + dudt1 * dt g1[i] = g1[i - 1] + dgdt1 * dt if v1[i] &gt;= vpeak: v1[i - 1] = vpeak v1[i] = c u1[i] = u1[i] + d spike1[i] = 1 # neuron 2 dvdt2 = (k * (v2[i - 1] - vr) * (v2[i - 1] - vt) - u2[i - 1] + w_12[trl] * g1[i - 1]) / C dudt2 = a * (b * (v2[i - 1] - vr) - u2[i - 1]) dgdt2 = (-g2[i - 1] + psp_amp * spike2[i - 1]) / psp_decay v2[i] = v2[i - 1] + dvdt2 * dt u2[i] = u2[i - 1] + dudt2 * dt g2[i] = g2[i - 1] + dgdt2 * dt if v2[i] &gt;= vpeak: v2[i - 1] = vpeak v2[i] = c u2[i] = u2[i] + d spike2[i] = 1 # update synaptic weights delta_w = update_weight_func() w_12[trl + 1] = w_12[trl] + delta_w # store trial info g_record[trl, :] = g v1_record[trl, :] = v1 g1_record[trl, :] = g1 v2_record[trl, :] = v2 g2_record[trl, :] = g2 plot_results() def update_weight_rl(): global trl, r_obtained, r_predicted delta[trl] = r_obtained[trl] - r_predicted[trl] r_predicted[trl + 1] = r_predicted[trl] + gamma * delta[trl] pre = g1.sum() post = g2.sum() delta_w = alpha * pre * post * delta[trl] return delta_w n_trials = 100 trl = 0 tau = 0.1 T = 100 t = np.arange(0, T, tau) n = t.shape[0] C = 50 vr = -80 vt = -25 vpeak = 40 k = 1 a = 0.01 b = -20 c = -55 d = 150 psp_amp = 1e5 psp_decay = 10 g = np.zeros(n) spike = np.zeros(n) spike[200:800:20] = 1 alpha = 3e-14 beta = 3e-14 gamma = 0.1 array_dict = init_arrays() r_predicted = array_dict[&#39;r_predicted&#39;] r_obtained = array_dict[&#39;r_obtained&#39;] delta = array_dict[&#39;delta&#39;] v1 = array_dict[&#39;v1&#39;] u1 = array_dict[&#39;u1&#39;] g1 = array_dict[&#39;g1&#39;] spike1 = array_dict[&#39;spike1&#39;] v2 = array_dict[&#39;v2&#39;] u2 = array_dict[&#39;u2&#39;] g2 = array_dict[&#39;g2&#39;] spike2 = array_dict[&#39;spike2&#39;] w_01 = array_dict[&#39;w_01&#39;] w_12 = array_dict[&#39;w_12&#39;] g_record = array_dict[&#39;g_record&#39;] v1_record = array_dict[&#39;v1_record&#39;] g1_record = array_dict[&#39;g1_record&#39;] v2_record = array_dict[&#39;v2_record&#39;] g2_record = array_dict[&#39;g2_record&#39;] update_weight_func = update_weight_rl simulate_network(update_weight_func) "],["example-2.html", "13 Example 2", " 13 Example 2 In the previous example, we simply assumed that the aggent received a reward with magnitude 1 on the first 1/4 and 3/4 of all trials. Therefore, whether or not the agent received a reward was independent of what the agent actually did. This means that the previous example was an instance of classical conditioning – think Pavlov’s dog. Here, we will simulate a scenario where whether or not the agent receives a reward depends on the action that it takes – i.e., we will examine an instance of instrumental conditioning. We will only need to update the simulate_network function by specifying how activity in the network should be mapped to actions. Our approach will be to assume that the network emits a response – e.g., presses a lever like a rat in an instrumental conditioning chamber – completely randomly. def simulate_network_inst(update_weight_func): global trl, r_obtained, r_predicted for j in range(n_trials - 1): trl = j for i in range(1, n): dt = t[i] - t[i - 1] # external input dgdt = (-g[i - 1] + psp_amp * spike[i - 1]) / psp_decay g[i] = g[i - 1] + dgdt * dt # neuron 1 dvdt1 = (k * (v1[i - 1] - vr) * (v1[i - 1] - vt) - u1[i - 1] + w_01[trl] * g[i - 1]) / C dudt1 = a * (b * (v1[i - 1] - vr) - u1[i - 1]) dgdt1 = (-g1[i - 1] + psp_amp * spike1[i - 1]) / psp_decay v1[i] = v1[i - 1] + dvdt1 * dt u1[i] = u1[i - 1] + dudt1 * dt g1[i] = g1[i - 1] + dgdt1 * dt if v1[i] &gt;= vpeak: v1[i - 1] = vpeak v1[i] = c u1[i] = u1[i] + d spike1[i] = 1 # neuron 2 dvdt2 = (k * (v2[i - 1] - vr) * (v2[i - 1] - vt) - u2[i - 1] + w_12[trl] * g1[i - 1]) / C dudt2 = a * (b * (v2[i - 1] - vr) - u2[i - 1]) dgdt2 = (-g2[i - 1] + psp_amp * spike2[i - 1]) / psp_decay v2[i] = v2[i - 1] + dvdt2 * dt u2[i] = u2[i - 1] + dudt2 * dt g2[i] = g2[i - 1] + dgdt2 * dt if v2[i] &gt;= vpeak: v2[i - 1] = vpeak v2[i] = c u2[i] = u2[i] + d spike2[i] = 1 # press lever / earn reward on a random 25% of all trials if np.random.uniform(0, 1) &gt; 0.25: r_obtained[trl] = 1 # update synaptic weights delta_w = update_weight_func() w_12[trl + 1] = w_12[trl] + delta_w # store trial info g_record[trl, :] = g v1_record[trl, :] = v1 g1_record[trl, :] = g1 v2_record[trl, :] = v2 g2_record[trl, :] = g2 plot_results() n_trials = 100 trl = 0 tau = 0.1 T = 100 t = np.arange(0, T, tau) n = t.shape[0] C = 50 vr = -80 vt = -25 vpeak = 40 k = 1 a = 0.01 b = -20 c = -55 d = 150 psp_amp = 1e5 psp_decay = 10 g = np.zeros(n) spike = np.zeros(n) spike[200:800:20] = 1 alpha = 3e-14 beta = 3e-14 gamma = 0.1 array_dict = init_arrays() r_predicted = array_dict[&#39;r_predicted&#39;] # NOTE: redefine r_obtained to be all zeros, so that the # network simulation can populate it on the fly # r_obtained = array_dict[&#39;r_obtained&#39;] r_obtained = np.zeros(n_trials) delta = array_dict[&#39;delta&#39;] v1 = array_dict[&#39;v1&#39;] u1 = array_dict[&#39;u1&#39;] g1 = array_dict[&#39;g1&#39;] spike1 = array_dict[&#39;spike1&#39;] v2 = array_dict[&#39;v2&#39;] u2 = array_dict[&#39;u2&#39;] g2 = array_dict[&#39;g2&#39;] spike2 = array_dict[&#39;spike2&#39;] w_01 = array_dict[&#39;w_01&#39;] w_12 = array_dict[&#39;w_12&#39;] g_record = array_dict[&#39;g_record&#39;] v1_record = array_dict[&#39;v1_record&#39;] g1_record = array_dict[&#39;g1_record&#39;] v2_record = array_dict[&#39;v2_record&#39;] g2_record = array_dict[&#39;g2_record&#39;] update_weight_func = update_weight_rl simulate_network_inst(update_weight_func) "],["example-3.html", "14 Example 3 14.1 Reinforcement learning framework 14.2 Estimating the state-value function 14.3 Temporal difference (TD) learning 14.4 TD(0) for estimating \\(v_{\\pi}\\) 14.5 SARSA for estimating \\(Q\\) 14.6 Q-learning for estimating \\(\\pi\\) 14.7 Dyna-Q: Model-based RL", " 14 Example 3 In the previous example, we programmed the model to make responses / earn rewards completely randomly. Next, lets give the model some more agency by allowing it to make responses / earn rewards whenever the output of neuron 2 crosses a threshold. def simulate_network_inst_2(update_weight_func): global trl, r_obtained, r_predicted for j in range(n_trials - 1): trl = j for i in range(1, n): dt = t[i] - t[i - 1] # external input dgdt = (-g[i - 1] + psp_amp * spike[i - 1]) / psp_decay g[i] = g[i - 1] + dgdt * dt # neuron 1 dvdt1 = (k * (v1[i - 1] - vr) * (v1[i - 1] - vt) - u1[i - 1] + w_01[trl] * g[i - 1]) / C dudt1 = a * (b * (v1[i - 1] - vr) - u1[i - 1]) dgdt1 = (-g1[i - 1] + psp_amp * spike1[i - 1]) / psp_decay v1[i] = v1[i - 1] + dvdt1 * dt u1[i] = u1[i - 1] + dudt1 * dt g1[i] = g1[i - 1] + dgdt1 * dt if v1[i] &gt;= vpeak: v1[i - 1] = vpeak v1[i] = c u1[i] = u1[i] + d spike1[i] = 1 # neuron 2 dvdt2 = (k * (v2[i - 1] - vr) * (v2[i - 1] - vt) - u2[i - 1] + w_12[trl] * g1[i - 1]) / C dudt2 = a * (b * (v2[i - 1] - vr) - u2[i - 1]) dgdt2 = (-g2[i - 1] + psp_amp * spike2[i - 1]) / psp_decay v2[i] = v2[i - 1] + dvdt2 * dt u2[i] = u2[i - 1] + dudt2 * dt g2[i] = g2[i - 1] + dgdt2 * dt if v2[i] &gt;= vpeak: v2[i - 1] = vpeak v2[i] = c u2[i] = u2[i] + d spike2[i] = 1 # press lever / earn reward on a random 25% of all trials if np.random.uniform(0, 1) &gt; 0.25: r_obtained[trl] = 1 # also press the lever / earn reward if neuron 2 is # sufficiently active if g2.sum() &gt; resp_thresh: r_obtained[trl] = 1 # update synaptic weights delta_w = update_weight_func() w_12[trl + 1] = w_12[trl] + delta_w # store trial info g_record[trl, :] = g v1_record[trl, :] = v1 g1_record[trl, :] = g1 v2_record[trl, :] = v2 g2_record[trl, :] = g2 plot_results() n_trials = 100 trl = 0 tau = 0.1 T = 100 t = np.arange(0, T, tau) n = t.shape[0] C = 50 vr = -80 vt = -25 vpeak = 40 k = 1 a = 0.01 b = -20 c = -55 d = 150 psp_amp = 1e5 psp_decay = 10 g = np.zeros(n) spike = np.zeros(n) spike[200:800:20] = 1 alpha = 3e-14 beta = 3e-14 gamma = 0.1 resp_thresh = 4e7 array_dict = init_arrays() r_predicted = array_dict[&#39;r_predicted&#39;] # NOTE: redefine r_obtained to be all zeros, so that the # network simulation can populate it on the fly # r_obtained = array_dict[&#39;r_obtained&#39;] r_obtained = np.zeros(n_trials) delta = array_dict[&#39;delta&#39;] v1 = array_dict[&#39;v1&#39;] u1 = array_dict[&#39;u1&#39;] g1 = array_dict[&#39;g1&#39;] spike1 = array_dict[&#39;spike1&#39;] v2 = array_dict[&#39;v2&#39;] u2 = array_dict[&#39;u2&#39;] g2 = array_dict[&#39;g2&#39;] spike2 = array_dict[&#39;spike2&#39;] w_01 = array_dict[&#39;w_01&#39;] w_12 = array_dict[&#39;w_12&#39;] g_record = array_dict[&#39;g_record&#39;] v1_record = array_dict[&#39;v1_record&#39;] g1_record = array_dict[&#39;g1_record&#39;] v2_record = array_dict[&#39;v2_record&#39;] g2_record = array_dict[&#39;g2_record&#39;] update_weight_func = update_weight_rl simulate_network_inst_2(update_weight_func) 14.1 Reinforcement learning framework An agent can occupy a discrete set of states \\(S\\) and can take a discrete set of actions \\(A\\) from each state as determined by some policy \\(\\pi\\). \\[ S = \\{s_1, s_2, \\ldots, s_n\\} \\\\ A = \\{a_1, a_2, \\ldots, a_n\\} \\\\ \\pi \\rightarrow P(a_i | s_j) \\] The actions \\(a_i\\) taken by the agent determine the probability that the state will transition from \\(s_i\\) to \\(s_j\\), and also determine the probability of current and future reward (because rewards are causally determined by states). The goal of the agent is to learn to take actions that maximize current and future reward. That is, the RL agent tries to determine the state-value function \\(V_{\\pi}(s)\\) of a given policy \\(\\pi\\) as a function of each state \\(s\\). \\[ \\begin{align} V_{\\pi}(s) &amp;= \\operatorname{E}[R | \\pi, s] \\nonumber \\\\ &amp;= \\operatorname{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t | \\pi, s \\right]. \\end{align} \\] Here, \\(\\gamma\\) is a temporal discounting factor that allows the agent to care more about near rewards than about distant future rewards. The fundamental goal of RL is to estimate \\(V_{\\pi}(s)\\) using nothing more than the experience an agent gains while navigating its environment. The specific way in which an RL algorithm does this varies greatly from one algorithm to the next, and is still very much an active area of research. 14.2 Estimating the state-value function RL aims to learn state-value function through experience. The general structure is to give the agent many trials of experience, where on each trial the agent visits some sequence of states by taking some sequence of actions. A trial of experience comes to an end whenever a terminal state is visited, but in principle, goes on forever until such a state is experienced. The layout of an RL program will generally look something like the following: Iterate over episodes Specify the policy \\(\\pi\\) to be evaluated Initialise \\(V(s)\\) Initialise \\(S\\) Iterate over steps per episodes \\(A \\leftarrow\\) action given by \\(\\pi\\) for \\(S\\) Take action \\(A\\), observe \\(R\\), \\(S&#39;\\) Update value function \\(V\\) \\(S \\leftarrow S&#39;\\) If \\(S = S_{\\text{terminal}}\\) then break Put in Python pseudo code: import numpy as np import matplotlib.pyplot as plt n_trials = 10 # specify the set of states states = np.array([1, 2, 3]) n_states = states.shape[0] # initialise value estimate v = np.zeros((n_trials, n_states)) for trial in range(n_trials): terminate = False while not terminate: # select action (using policy pi) # transition to new state (determined by environment) # possibly receive reward (determined by environment) # update value estimate of newly arrived in state # (variety of RL algorithms for this -- e.g., TD # learning (see below)) # check if current state is terminal # if s == 3: # terminate = True # for now just set to True to avoid infinite while loop terminate = True 14.3 Temporal difference (TD) learning TD RL estimates the state-value function under the assumption that the action policy is fixed. The term temporal in TD learning refers to the difference between successive visits to a particular state (across trials), not necessarily across different times within a trial. TD learning simply tries to iteratively update its estimate of a states value by directly experiencing them, comparing what was experienced to what was expected, and updating its expectation to more closely match recent experience. Let \\(n\\) index the current trial, \\(s\\) be the state just arrived in, \\(s&#39;\\) be the next future state (knowable because TD assumes a fixed action selection policy), and \\(\\hat{V}_{n}(s)\\) be the state-value function estimate on trial \\(n\\) of state \\(s\\). TD learning updates the state-value function across trials as follows: \\[ \\begin{equation} \\hat{V}_{n}(s) = \\hat{V}_{n-1}(s) + \\alpha (r_{n}(s) + \\gamma \\hat{V}_{n-1}(s&#39;) - \\hat{V}_{n-1}(s)). \\end{equation} \\] The term \\(r_{n}\\) is the reward that was delivered upon arrival to the current state \\(s&#39;\\). \\(\\hat{V}_{n-1}(s&#39;)\\) is the reward that is expected to arrive in future states visited beyond \\(s&#39;\\). This value has to come from the the state-value function estimate in the previous trial \\(n-1\\) because the agent has not yet experienced states beyond \\(s&#39;\\) in the current trial. \\(\\alpha\\) is called the learning rate and \\(\\gamma\\) is the temporal discounting parameter. Notice that you can write the value update as follows: \\[ \\begin{equation} \\hat{V}_{n}(s) = (1-\\alpha) (\\hat{V}_{n-1}(s)) + \\alpha (r_{n}(s) + \\gamma \\hat{V}_{n-1}(s&#39;)). \\end{equation} \\] In this form it may be easier to see that the update to our estimate of the state-value function is a weighted average of whatever it was on the previous trial with whatever current reward was experienced and future reward is expected from the newly arrived in state \\(s&#39;\\). In the psuedo code notation used by your RL book it looks like this: Iterate over episodes Specify the policy \\(\\pi\\) to be evaluated Initialise \\(V(s)\\) Initialise \\(S\\) Iterate over steps per episodes \\(A \\leftarrow\\) action given by \\(\\pi\\) for \\(S\\) Take action \\(A\\), observe \\(R\\), \\(S&#39;\\) \\(V(S) \\leftarrow V(S) + \\alpha \\left[ R + \\gamma V(S&#39;) - V(S) \\right]\\) \\(S \\leftarrow S&#39;\\) If \\(S = S_{\\text{terminal}}\\) then break 14.3.1 TD RL model of classical conditioning Consider the classic Pavlov’s dog experiment. States are taken to be time steps between cue onset and reward delivery, which occurs on every trial at time step \\(T\\) with magnitude \\(r\\). The value update equation then becomes: \\[ \\begin{equation} \\hat{V}_{n}(t) = \\hat{V}_{n-1}(t) + \\alpha (r_{n}(t) + \\gamma \\hat{V}_{n-1}(t+1) - \\hat{V}_{n-1}(t)) \\end{equation} \\] To get a feel for how this works, consider the first few trials. \\(n=1, t=T\\) _{0} for all \\(t\\) by assumption of initial conditions. \\(r_{1}(T)=r\\) because the dog receives a reward on each trial at time \\(T\\). This leads to the following: \\[ \\begin{align} \\hat{V}_{1}(T) &amp;= \\hat{V}_{0}(t) + \\alpha [r_{1}(T) + \\gamma \\hat{V}_{0}(T+1) - \\hat{V}_{0}(T)] \\\\ &amp;= \\alpha r \\\\\\\\ \\end{align} \\] \\(n=2, t=T-1\\) \\(\\hat{V}_{1}(T-1)=0\\) because at time \\(T-1\\) of trial \\(1\\) the agent has not yet received any rewards. \\(r_{2}(T-1)=0\\) because rewards are delivered only at time \\(T\\), not at time \\(T-1\\). \\(\\hat{V}_2(T-1)=\\alpha r\\) as we showed above. This leads to the following: \\[ \\begin{align} \\hat{V}_{2}(T-1) &amp;= \\hat{V}_{1}(T-1) + \\alpha [r_{2}(T-1)+ \\gamma \\hat{V}_{1}(T) - \\hat{V}_{1}(T-1)] \\\\ &amp;= \\alpha^2 \\gamma r \\end{align} \\] In other words, the reward prediction error \\(\\delta\\) that occurred at time \\(T\\) on trial \\(1\\) has propagated back on trial \\(2\\) to the immediately preceding state (i.e., \\(T−1\\)). Similarly, on trial \\(3\\), the positive value associated with state \\(T-1\\) will propagate back to state \\(T-2\\). In this way, the value associated with earlier and earlier states will increase. This propagation will continue until it eventually reaches the time of cue presentation – that is, until \\(\\hat{V}_{n}(0) &gt; 0\\), for some value of \\(n\\). It will not propagate to earlier times than this however, so long as cue presentation times are unpredictable. We can implement this simple system in python code as follows: import numpy as np import matplotlib.pyplot as plt n_trials = 1000 n_steps = 100 v_init = 0.0 alpha = 0.1 gamma = 1 v = np.zeros((n_steps, n_trials)) v[:, 0] = v_init for n in range(1, n_trials): for t in range(n_steps - 1): s = t sprime = t + 1 r = 1 if s == (n_steps - 2) else 0 v[s, n] = v[s, n - 1] + alpha * (r + gamma * v[sprime, n - 1] - v[s, n - 1]) fig, ax = plt.subplots(1, 1, squeeze=False) pos = ax[0, 0].imshow(v, aspect=&#39;auto&#39;) ax[0, 0].set_xlabel(&#39;trial&#39;) ax[0, 0].set_ylabel(&#39;time step (state)&#39;) cbar = fig.colorbar(pos, ax=ax[0, 0]) cbar.ax.get_yaxis().labelpad = 15 cbar.ax.set_ylabel(&#39;Value function estimate&#39;, rotation=270) plt.show() 14.3.2 TD RL as the learning signal in a spiking network Schultz, W., Dayan, P., &amp; Montague, P. R. (1997). A Neural Substrate of Prediction and Reward. Science, 275(5306), 1593–1599. In my opinion, one of the coolest ever stories to emerge from computational neuroscience is the idea that dopamine encodes the reward prediction error of TD RL system, and that this prediction error signal drives learning in the basal ganglia. Here, we will consider a simple spiking network consisting of two neurons \\(A \\rightarrow B\\) in which the synaptic strength connecting them is learned using the TD RL reward prediction error. import numpy as np import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec def init_arrays(): resp = np.zeros((n, n_trials)) # TD arrays v = np.zeros((n, n_trials)) r = np.zeros((n, n_trials)) rpe = np.zeros((n, n_trials)) v1 = np.zeros(n) u1 = np.zeros(n) g1 = np.zeros(n) spike1 = np.zeros(n) v1[0] = vr v2 = np.zeros(n) u2 = np.zeros(n) g2 = np.zeros(n) spike2 = np.zeros(n) v2[0] = vr w_01 = 0.4 * np.ones(n_trials) # needs to be 2D since it changes every time step w_12 = 0.4 * np.ones((n, n_trials)) g_record = np.zeros((n_trials, n)) v1_record = np.zeros((n_trials, n)) g1_record = np.zeros((n_trials, n)) v2_record = np.zeros((n_trials, n)) g2_record = np.zeros((n_trials, n)) return { &#39;resp&#39;: resp, &#39;v&#39;: v, &#39;r&#39;: r, &#39;rpe&#39;: rpe, &#39;v1&#39;: v1, &#39;u1&#39;: u1, &#39;g1&#39;: g1, &#39;spike1&#39;: spike1, &#39;v2&#39;: v2, &#39;u2&#39;: u2, &#39;g2&#39;: g2, &#39;spike2&#39;: spike2, &#39;w_01&#39;: w_01, &#39;w_12&#39;: w_12, &#39;g_record&#39;: g_record, &#39;v1_record&#39;: v1_record, &#39;g1_record&#39;: g1_record, &#39;v2_record&#39;: v2_record, &#39;g2_record&#39;: g2_record } def simulate_network_inst_TD(): global trl, r_obtained, r_predicted for j in range(n_trials - 1): trl = j # press lever / earn reward on a random % of all trials if np.random.uniform(0, 1) &gt; 0.95: # press lever / earn reward at the end of the trial resp[-2, trl] = 1 r[-2, trl] = 1 for i in range(1, n - 1): dt = t[i] - t[i - 1] # external input dgdt = (-g[i - 1] + psp_amp * spike[i - 1]) / psp_decay g[i] = g[i - 1] + dgdt * dt # neuron 1 dvdt1 = (k * (v1[i - 1] - vr) * (v1[i - 1] - vt) - u1[i - 1] + w_01[trl] * g[i - 1]) / C dudt1 = a * (b * (v1[i - 1] - vr) - u1[i - 1]) dgdt1 = (-g1[i - 1] + psp_amp * spike1[i - 1]) / psp_decay v1[i] = v1[i - 1] + dvdt1 * dt u1[i] = u1[i - 1] + dudt1 * dt g1[i] = g1[i - 1] + dgdt1 * dt if v1[i] &gt;= vpeak: v1[i - 1] = vpeak v1[i] = c u1[i] = u1[i] + d spike1[i] = 1 # neuron 2 dvdt2 = (k * (v2[i - 1] - vr) * (v2[i - 1] - vt) - u2[i - 1] + w_12[i - 1, trl] * g1[i - 1]) / C dudt2 = a * (b * (v2[i - 1] - vr) - u2[i - 1]) dgdt2 = (-g2[i - 1] + psp_amp * spike2[i - 1]) / psp_decay v2[i] = v2[i - 1] + dvdt2 * dt u2[i] = u2[i - 1] + dudt2 * dt g2[i] = g2[i - 1] + dgdt2 * dt if v2[i] &gt;= vpeak: v2[i - 1] = vpeak v2[i] = c u2[i] = u2[i] + d spike2[i] = 1 # check for response if none has been made yet if resp[:, trl].sum() == 0: # press the lever / earn reward if neuron 2 is sufficiently active if g2.sum() &gt; resp_thresh: resp[i, trl] = 1 r[-2, trl] = 1 # update TD value function estimate rpe[i, trl] = r[i, trl] + gamma * v[i + 1, trl - 1] - v[i, trl - 1] v[i, trl] = v[i, trl - 1] + alpha * rpe[i, trl] # update synaptic weights delta_w = alpha_w * rpe[i, trl] w_12[i + 1, trl] = w_12[i, trl] + delta_w # store trial info g_record[trl, :] = g v1_record[trl, :] = v1 g1_record[trl, :] = g1 v2_record[trl, :] = v2 g2_record[trl, :] = g2 # plot_results fig, ax = plt.subplots(2, 2, squeeze=False) ax.flatten()[0].imshow(v, aspect=&#39;auto&#39;) ax.flatten()[1].imshow(rpe, aspect=&#39;auto&#39;) ax.flatten()[2].imshow(w_12, aspect=&#39;auto&#39;) ax.flatten()[3].spy(resp, aspect=&#39;auto&#39;, marker=&#39;o&#39;, markersize=5) ax.flatten()[0].set_title(&#39;value function estimate&#39;) ax.flatten()[1].set_title(&#39;reward prediction error&#39;) ax.flatten()[2].set_title(&#39;synaptic weight&#39;) ax.flatten()[3].set_title(&#39;response&#39;) [x.set_xlabel(&#39;trial&#39;) for x in ax.flatten()] [x.set_ylabel(&#39;time step&#39;) for x in ax.flatten()] plt.tight_layout() plt.show() n_trials = 1000 trl = 0 tau = 0.1 T = 100 t = np.arange(0, T, tau) n = t.shape[0] C = 50 vr = -80 vt = -25 vpeak = 40 k = 1 a = 0.01 b = -20 c = -55 d = 150 psp_amp = 1e5 psp_decay = 10 g = np.zeros(n) spike = np.zeros(n) spike[200:800:20] = 1 # the _w notation is to distinguish from the TD params below alpha_w = 2e1 # TD params v_init = 0.0 alpha = 0.25 gamma = 1 resp_thresh = 3e5 array_dict = init_arrays() resp = array_dict[&#39;resp&#39;] rpe = array_dict[&#39;rpe&#39;] v = array_dict[&#39;v&#39;] r = array_dict[&#39;r&#39;] v1 = array_dict[&#39;v1&#39;] u1 = array_dict[&#39;u1&#39;] g1 = array_dict[&#39;g1&#39;] spike1 = array_dict[&#39;spike1&#39;] v2 = array_dict[&#39;v2&#39;] u2 = array_dict[&#39;u2&#39;] g2 = array_dict[&#39;g2&#39;] spike2 = array_dict[&#39;spike2&#39;] w_01 = array_dict[&#39;w_01&#39;] w_12 = array_dict[&#39;w_12&#39;] g_record = array_dict[&#39;g_record&#39;] v1_record = array_dict[&#39;v1_record&#39;] g1_record = array_dict[&#39;g1_record&#39;] v2_record = array_dict[&#39;v2_record&#39;] g2_record = array_dict[&#39;g2_record&#39;] simulate_network_inst_TD() 14.4 TD(0) for estimating \\(v_{\\pi}\\) TD(0) is about prediction, not about control You can see this in the algorithm description below in that the policy \\(\\pi\\) is fixed and specified at the top of the program. Iterate over episodes Specify the policy \\(\\pi\\) to be evaluated Initialise \\(V(s)\\) Initialise \\(S\\) Iterate over steps per episodes \\(A \\leftarrow\\) action given by \\(\\pi\\) for \\(S\\) Take action \\(A\\), observe \\(R\\), \\(S&#39;\\) \\(V(S) \\leftarrow V(S) + \\alpha \\left[ R + \\gamma V(S&#39;) - V(S) \\right]\\) \\(S \\leftarrow S&#39;\\) If \\(S = S_{\\text{terminal}}\\) then break 14.4.1 TD in a simple 2-arm bandit task This is a very simple scenario in which the agent begins in state \\(s_0\\) and can select only one of two actions. Action \\(a_1\\) selects the slot machine on the left and leads to state \\(s_l\\), and action \\(a_2\\) selects the slot machine on the right and leads to state \\(s_r\\). Reward is delivered in state \\(s_l\\) and \\(s_r\\) with different probability, and both are terminal states. Let \\(n\\) index the current trial and \\(\\hat{V}_{n}(s)\\) be the state-value function estimate on trial \\(n\\) of state \\(s\\in\\{s_l,s_r\\}\\). In the 2-armed bandit task descirbed above, TD iteratively updates its estimate of \\(\\hat{V}_{n}(s)\\) according to the following: \\[ \\begin{equation} \\hat{V}_{n}(s) = \\hat{V}_{n-1}(s) + \\alpha (r_{n} - \\hat{V}_{n-1}(s)). \\end{equation} \\] The rightmost term \\(r_{n} - \\hat{V}_{n-1}(s)\\) is called the reward prediction error (RPE). Conceptually, RPE is simply the difference between the obtained and expected reward. It is easy to see that learning a good estimate of the value function is equivalent to eliminating RPE. RPE is often notated as \\(\\delta\\), so we can write \\(\\delta_{n}=r_{n}-\\hat{V}_{n-1}(s)\\). You can also write the value update equation in the following form: \\[ \\begin{equation} \\hat{V}_{n}(s) = (1-\\alpha) \\hat{V}_{n-1}(s) + \\alpha r_{n}. \\end{equation} \\] In this form it may be easier to see that the update to our estimate of the state-value function is a weighted average of whatever it was on the previous trial with whatever current reward was experienced. In code, a TD agent performing a 2-armed bandit task in which it simply chooses which bandit to select at random looks as follows: import numpy as np import matplotlib.pyplot as plt n_trials = 1000 v_init = 0.5 p_reward_1 = 7 p_reward_2 = 6 alpha = 0.01 epsilon = 0.2 v = np.zeros((2, n_trials)) v[:, 0] = v_init for i in range(0, n_trials - 1): # action selection - guessing if np.random.uniform() &lt; 0.5: # reward r = np.random.normal(p_reward_1, 2) # reward prediction error delta = r - v[0, i] # value update v[0, i + 1] = v[0, i] + alpha * delta v[1, i + 1] = v[1, i] else: # reward r = np.random.normal(p_reward_2, 2) # reward prediction error delta = r - v[1, i] # value update v[1, i + 1] = v[1, i] + alpha * delta v[0, i + 1] = v[0, i] fig, ax = plt.subplots(1, 1, squeeze=False) ax[0, 0].plot(v[0, :], label=&#39;value 1&#39;) ax[0, 0].plot(v[1, :], label=&#39;value 2&#39;) plt.legend() plt.show() 14.4.2 Action selection policy We saw above that even if the agent simply guesses at each bandit, never modifying its action selection strategy to reflect its updating beliefs about the value of the the two options, the estimate of the value function still approaches the true value. This makes clear that some amount of guessing (i.e., exploration) is good for learning the value function, but perhaps not so great for actually maximising the obtained rewards (the actual goal of an RL agent). Two popular action selection policies – epsilon greedy (\\(\\epsilon\\)-greedy) and softman – attempt to balance exploration with exploitation. 14.4.2.1 Epsilon greedy import numpy as np import matplotlib.pyplot as plt n_trials = 1000 v_init = 0.5 p_reward_1 = 7 p_reward_2 = 6 alpha = 0.01 epsilon = 0.2 v = np.zeros((2, n_trials)) v[:, 0] = v_init for i in range(0, n_trials - 1): # action selection - greedy epsilon if np.random.uniform() &lt; epsilon: a = np.round(np.random.uniform()) else: a = np.argmax(v[:, i]) if a == 0: # reward r = np.random.normal(p_reward_1, 2) # reward prediction error delta = r - v[0, i] # value update v[0, i + 1] = v[0, i] + alpha * delta v[1, i + 1] = v[1, i] else: # reward r = np.random.normal(p_reward_2, 2) # reward prediction error delta = r - v[1, i] # value update v[1, i + 1] = v[1, i] + alpha * delta v[0, i + 1] = v[0, i] fig, ax = plt.subplots(1, 1, squeeze=False) ax[0, 0].plot(v[0, :], label=&#39;value 1&#39;) ax[0, 0].plot(v[1, :], label=&#39;value 2&#39;) plt.legend() plt.show() 14.4.2.2 Softmax import numpy as np import matplotlib.pyplot as plt n_trials = 1000 v_init = 0.5 p_reward_1 = 7 p_reward_2 = 6 alpha = 0.01 epsilon = 0.2 v = np.zeros((2, n_trials)) v[:, 0] = v_init for i in range(0, n_trials - 1): # action selection - softmax sm = np.exp(v[:, i]) / np.sum(np.exp(v[:, i])) if np.random.uniform() &lt; sm[0]: # reward r = np.random.normal(p_reward_1, 2) # reward prediction error delta = r - v[0, i] # value update v[0, i + 1] = v[0, i] + alpha * delta v[1, i + 1] = v[1, i] else: # reward r = np.random.normal(p_reward_2, 2) # reward prediction error delta = r - v[1, i] # value update v[1, i + 1] = v[1, i] + alpha * delta v[0, i + 1] = v[0, i] fig, ax = plt.subplots(1, 1, squeeze=False) ax[0, 0].plot(v[0, :], label=&#39;value 1&#39;) ax[0, 0].plot(v[1, :], label=&#39;value 2&#39;) plt.legend() plt.show() 14.5 SARSA for estimating \\(Q\\) SARSA – state-action-reward-state-action – attempts to learn \\(Q(s, a)\\), called the action-value function, which represents the value (or quality hence the \\(Q\\)) of taking action \\(a\\) in state \\(s\\). The policy that controls behaviour is derived from \\(Q\\) and that makes SARSA about both prediction and control. SARSA is called on policy because the only \\(Q\\) values that are learned about are those that correspond to state-action pairs that were directly experienced. Iterate over episodes Initialise \\(S\\) Choose \\(A\\) from \\(S\\) using policy derived from \\(Q\\) (e.g., \\(\\epsilon\\)-greedy) Iterate over steps per episodes Take action \\(A\\), observe \\(R\\), \\(S&#39;\\) Choose \\(A&#39;\\) from \\(S&#39;\\) using policy derived from \\(Q\\) (e.g., \\(\\epsilon\\)-greedy) \\(Q(S, A) \\leftarrow Q(S, A) + \\alpha \\left[ R + \\gamma Q(S&#39;, A&#39;) - Q(S, A) \\right]\\) \\(S \\leftarrow S&#39;\\), \\(A \\leftarrow A&#39;\\) If \\(S = S_{\\text{terminal}}\\) then break 14.6 Q-learning for estimating \\(\\pi\\) Like SARSA, Q-learning attempts to learn \\(Q(s, a)\\). Again like SARSA, Q-learning is about both prediction and control. Q-learning is called off policy because the \\(Q\\) values that are learned about are not necessarily only those that correspond to state-action pairs that were directly experienced. Initialise \\(Q(s, a)\\) Iterate over episodes Initialise \\(S\\) Iterate over steps per episodes Choose \\(A\\) from \\(S\\) using policy derived from \\(Q\\) (e.g., \\(\\epsilon\\)-greedy) Take action \\(A\\), observe \\(R\\), \\(S&#39;\\) \\(Q(S, A) \\leftarrow Q(S, A) + \\alpha \\left[ R + \\gamma \\max_{a} Q(S&#39;, a) - Q(S, A) \\right]\\) \\(S \\leftarrow S&#39;\\) If \\(S = S_{\\text{terminal}}\\) then break 14.6.1 Q-learning applied to instrumental conditioning import numpy as np import matplotlib.pyplot as plt n_episodes = 100 n_steps = 10 n_states = 6 n_actions = 3 alpha = 0.1 # initialise q(s,a) q = np.ones((n_states, n_actions)) * 0.5 # iterate over episodes for e in range(n_episodes): # initialise s s = 0 # iterate over steps per episodes for t in range(n_steps): # choose a from s using policy derived from q # here, we use softmax sm = np.exp(q[s, :]) / np.sum(np.exp(q[s, :])) a = np.random.choice([0, 1, 2], size=1, p=np.squeeze(sm)) # take action a, observe r, s&#39; if s==0: # press lever if a == 0: r = 0 sprime = 1 # pull chain elif a == 1: r = 0 sprime = 2 # enter magazine elif a == 2: r = 0 sprime = 3 elif s==1: # press lever if a == 0: r = 0 sprime = 3 # pull chain elif a == 1: r = 0 sprime = 3 # enter magazine elif a == 2: r = 1 sprime = 4 elif s==2: # press lever if a == 0: r = 0 sprime = 3 # pull chain elif a == 1: r = 0 sprime = 3 # enter magazine elif a == 2: r = 1 sprime = 5 # update q-function q[s, a] += alpha * (r + np.max(q[sprime, :]) - q[s, a]) # reset state s = sprime # stop if s is terminal if s == 3 or s == 4 or s == 5: break fig, ax = plt.subplots(1, 1, squeeze=False) ax[0,0].imshow(q) ax[0, 0].set_xlabel(&#39;action&#39;) ax[0, 0].set_ylabel(&#39;state&#39;) plt.show() 14.6.2 Q-learning applied to instrumental conditioning 2 import numpy as np import matplotlib.pyplot as plt n_episodes = 100 n_steps = 10 n_states = 6 n_actions = 3 alpha = 0.1 # initialise q(s,a) q = np.ones((n_states, n_actions)) * 0.5 # states S = np.arange(0, 6, 1) # Actions A = np.array([0, 1, 2]) # state transition probabilities T = np.zeros((n_states, n_actions, n_states)) T[0, 0, 1] = 1 # press lever transition to state 1 T[0, 1, 2] = 1 # pull chain transition to state 2 T[0, 2, 3] = 1 # enter magazine terminal no reward T[1, 0, 3] = 1 # press lever terminal no reward T[1, 1, 3] = 1 # pull chain terminal no reward T[1, 2, 4] = 1 # enter magazine terminal reward T[2, 0, 3] = 1 # press lever terminal no reward T[2, 1, 3] = 1 # pull chain terminal no reward T[2, 2, 5] = 1 # enter magazine terminal reward # state rewards R = np.zeros(n_states) R[4] = 1 R[5] = 1 # iterate over episodes for e in range(n_episodes): # initialise s s = 0 # iterate over steps per episodes for t in range(n_steps): # choose a from s using policy derived from q # here, we use softmax sm = np.exp(q[s, :]) / np.sum(np.exp(q[s, :])) a = np.random.choice(A, size=1, p=np.squeeze(sm)) # take action a, observe r, s&#39; sprime = np.random.choice(S, size=1, p=np.squeeze(T[s, a, :])) r = R[sprime] # update q-function q[s, a] += alpha * (r + np.max(q[sprime, :]) - q[s, a]) # reset state s = sprime # stop if s is terminal if s == 3 or s == 4 or s == 5: break fig, ax = plt.subplots(1, 1, squeeze=False) ax[0,0].imshow(q) ax[0, 0].set_xlabel(&#39;action&#39;) ax[0, 0].set_ylabel(&#39;state&#39;) plt.show() 14.7 Dyna-Q: Model-based RL Initialise \\(Q(s, a)\\), \\(\\widehat{Q}(s, a)\\), and \\(\\widehat{R}(s)\\) Iterate over episodes Initialise \\(S\\) Iterate over steps per episodes Choose \\(A\\) from \\(S\\) using policy derived from \\(Q\\) (e.g., \\(\\epsilon\\)-greedy) Take action \\(A\\), observe \\(R\\), \\(S&#39;\\) \\(Q(S, A) \\leftarrow Q(S, A) + \\alpha \\left[ R + \\gamma \\max_{a} Q(S&#39;, a) - Q(S, A) \\right]\\) Iterate over model-based episodes \\(S \\leftarrow\\) random previously observed state \\(A \\leftarrow\\) random previously taken action from state \\(S\\) \\(S&#39; \\leftarrow\\) sampled with probability \\(\\widehat{T}(S, A, S&#39;)\\) \\(R \\leftarrow \\widehat{R}(S)\\) \\(Q(S, A) \\leftarrow Q(S, A) + \\alpha \\left[ R + \\gamma \\max_{a} Q(S&#39;, a) - Q(S, A) \\right]\\) \\(S \\leftarrow S&#39;\\) If \\(S = S_{\\text{terminal}}\\) then break 14.7.1 Dyna-Q applied to instrumental conditioning import numpy as np import matplotlib.pyplot as plt n_episodes = 100 n_steps = 10 n_states = 6 n_actions = 3 alpha = 0.1 # initialise q(s,a) q = np.ones((n_states, n_actions)) * 0.5 # states S = np.arange(0, 6, 1) # Actions A = np.array([0, 1, 2]) # state transition probabilities T = np.zeros((n_states, n_actions, n_states)) T[0, 0, 1] = 1 # press lever transition to state 1 T[0, 1, 2] = 1 # pull chain transition to state 2 T[0, 2, 3] = 1 # enter magazine terminal no reward T[1, 0, 3] = 1 # press lever terminal no reward T[1, 1, 3] = 1 # pull chain terminal no reward T[1, 2, 4] = 1 # enter magazine terminal reward T[2, 0, 3] = 1 # press lever terminal no reward T[2, 1, 3] = 1 # pull chain terminal no reward T[2, 2, 5] = 1 # enter magazine terminal reward # state rewards R = np.zeros(n_states) R[4] = 1 R[5] = 1 # model of the environment n = 10 T_hat = np.zeros((n_states, n_actions, n_states)) R_hat = np.zeros(n_states) S_past = np.array([]) A_past = np.ones((n_states, n_actions)) * -1 # iterate over episodes for e in range(n_episodes): # initialise s s = 0 # iterate over steps per episodes for t in range(n_steps): # choose a from s using policy derived from q # here, we use softmax sm = np.exp(q[s, :]) / np.sum(np.exp(q[s, :])) a = np.random.choice(A, size=1, p=np.squeeze(sm))[0] # take action a, observe r, s&#39; sprime = np.random.choice(S, size=1, p=np.squeeze(T[s, a, :]))[0] r = R[sprime] # update q-function q[s, a] += alpha * (r + np.max(q[sprime, :]) - q[s, a]) # update models of the environment (tabular Dyna-Q p. 164) # assuming deterministic environment T_hat[s, a, sprime] = 1 R_hat[sprime] = r # keep track of experienced states and actions S_past = np.append(S_past, [s]) S_past = np.unique(S_past) A_past[s, a] = 1 # Simulate experience for i in range(n): # pick a previously experienced state s = np.random.choice(S_past, size=1)[0].astype(int) # select an action previously taken from state s eligible_actions = A_past[s, :] == 1 a = np.random.choice(np.where(eligible_actions)[0], size=1)[0] # simulate the outcome sprime_sim = np.random.choice(S, size=1, p=np.squeeze(T_hat[s, a, :]))[0] r = R[sprime_sim] # update the real Q function on the basis of the simulated outcome q[s, a] += alpha * (r + np.max(q[sprime_sim, :]) - q[s, a]) # reset state s = sprime # stop if s is terminal if s == 3 or s == 4 or s == 5: break fig, ax = plt.subplots(1, 1, squeeze=False) ax[0, 0].imshow(q) ax[0, 0].set_xlabel(&#39;action&#39;) ax[0, 0].set_ylabel(&#39;state&#39;) plt.show() "],["supervised-learning.html", "15 supervised learning", " 15 supervised learning import numpy as np import matplotlib.pyplot as plt # This week we will begin covering **supervised learning** # Hebbian learning: only depends on pre and post (no feedback signal) # Reinforcement learning: depends on pre and post but also a **how good / how # bad** feedback signal # Supervised learning: depends on pre post but also a **precise and # directional** feedback signal. # The best place to start looking at this is through the lens of so-called # state-space models of sensorimotor adaptation. # TODO: Here, go through and show was a typical visuomotor adaptation # experiment looks like. Emphasise that the modelling we will apply to this # behaviour is trial-by-trial (not moment-by-moment as in our neuron models). # define the rotation applied to the cursor on each trial r1 = np.zeros(100) r2 = np.ones(200) r3 = np.zeros(100) r = np.concatenate((r1, r2, r3)) # define total number of trials n = r.shape[0] # define an array to index trials t = np.arange(0, n, 1) # inspect the rotation that we just built # plt.plot(t, r) # plt.show() # Define an array to hold on to **state** values: # The **state** of the system is the mapping from goal reach location to motor # command that the system thinks will hit that goal. x = np.zeros(n) # Define the parameters that dictate how the state is learned # Learning rate (alpha): Determines how much is the new state influenced by the # last error signal. # Retention rate (beta): Determines how much is the new state influenced by the # previous state value. # alpha and beta need not sum to 1. alpha = 0.075 beta = 0.9 # Define an array to hold on to **error** values: # The **error** of the system is the signed and graded differences between # where the system was trying to reach, and where it actually reached. delta = np.zeros(n) # simulate the system for i in range(1, n): delta[i-1] = r[i-1] - x[i-1] x[i] = beta * x[i-1] + alpha * delta[i-1] # inspect results plt.plot(t, r, &#39;-k&#39;) plt.plot(t, delta, label=&#39;error&#39;) plt.plot(t, x, label=&#39;state value&#39;) plt.legend() plt.show() "],["supervised-learning-1.html", "16 Supervised Learning", " 16 Supervised Learning import numpy as np import matplotlib.pyplot as plt # NOTE: Example 1 # Here, we build a fully interconnected 2-layer artificial neural network. We # suppose that layer 1 has random activation but is constant over trials. We # let our goal be for layer 2 to have its first half elements at 1 and its # second half elements at zero. n = 10 x1 = np.random.uniform(0, 1, n) x2 = np.zeros(n) w12 = np.random.uniform(0, 1, (x1.shape[0], x2.shape[0])) delta_w12 = np.zeros(w12.shape) x2_desired = np.zeros(n) x2_desired[:n // 2] = 1 alpha = 0.1 n_trials = 100 for i in range(n_trials): # compute activation in x2 units for j in range(x2.shape[0]): for k in range(x1.shape[0]): y = w12[k, j] * x1[k] x2[j] += y delta_w12[k, j] = y - x2_desired[j] x2[j] = 1 / (1 + np.exp(-(x2[j] - 0.5) * 20)) # update weights against the error gradient for j in range(x2.shape[0]): for k in range(x1.shape[0]): w12[k, j] += -alpha * delta_w12[k, j] # plt.plot(x2, label=&#39;actual&#39;) # plt.plot(x2_desired, label=&#39;desired&#39;) # plt.legend() # plt.show() x2[:] = 0 # NOTE: Example 2 # Here, we build a fully interconnected 3-layer artificial neural network. We # suppose that layer 1 has random activation but is constant over trials. We # let our goal be for layer 3 to have its first half elements at 1 and its # second half elements at zero. Layer 2 can be anything it needs to be meet the # goal of layer 3. n = 10 x1 = np.random.uniform(0, 1, n) x2 = np.zeros(n) x3 = np.zeros(n) w12 = np.random.uniform(0, 1, (x1.shape[0], x2.shape[0])) w23 = np.random.uniform(0, 1, (x2.shape[0], x3.shape[0])) delta_w12 = np.zeros(w12.shape) delta_w23 = np.zeros(w12.shape) x3_desired = np.zeros(n) x3_desired[:n // 2] = 1 alpha = 0.2 n_trials = 500 for i in range(n_trials): # compute activation in x2 units for j in range(x2.shape[0]): for k in range(x1.shape[0]): x2[j] += w12[k, j] * x1[k] x2[j] = 1 / (1 + np.exp(-(x2[j] - 0.5) * 20)) # compute activation in x3 units for j in range(x3.shape[0]): for k in range(x2.shape[0]): x3[j] += w23[k, j] * x2[k] x3[j] = 1 / (1 + np.exp(-(x3[j] - 0.5) * 20)) # compute error gradients for j in range(x3.shape[0]): for k in range(x2.shape[0]): for l in range(x1.shape[0]): y = w23[k, j] * w12[l, k] * x1[l] delta = y - x3_desired[j] delta_w12[l, k] = w23[k, j] * x1[l] * delta delta_w23[k, j] = w12[l, k] * x1[l] * delta # update weights against the error gradient for j in range(x3.shape[0]): for k in range(x2.shape[0]): w23[k, j] += -alpha * delta_w23[k, j] for k in range(x2.shape[0]): for l in range(x1.shape[0]): w12[l, k] += -alpha * delta_w12[l, k] # plt.plot(x1, label=&#39;x1&#39;) # plt.plot(x2, label=&#39;x2&#39;) # plt.plot(x3, label=&#39;x3&#39;) # plt.plot(x3_desired, label=&#39;x3 desired&#39;) # plt.legend() # plt.show() x2[:] = 0 x3[:] = 0 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
