## TD(0) for estimating $v_{\pi}$

* TD(0) is about *prediction*, not about *control*

* You can see this in the algorithm description below in
that the policy $\pi$ is fixed and specified at the top of
the program.

***

<div style="border: 1px solid black; background: lightgrey;">

* Iterate over episodes

  * Specify the policy $\pi$ to be evaluated
  
  * Initialise $V(s)$
  
  * Initialise $S$

  * Iterate over steps per episodes

    * $A \leftarrow$ action given by $\pi$ for $S$ 
    
    * Take action $A$, observe $R$, $S'$
    
    * $V(S) \leftarrow V(S) + \alpha \left[ R + \gamma V(S') - V(S) \right]$
    
    * $S \leftarrow S'$
    
    * If $S = S_{\text{terminal}}$ then break 
    
</div>

***

### TD in a simple 2-arm bandit task

* This is a very simple scenario in which the agent begins
in state $s_0$ and can select only one of two actions.
Action $a_1$ selects the slot machine on the left and leads
to state $s_l$, and action $a_2$ selects the slot machine on
the right and leads to state $s_r$. Reward is delivered in
state $s_l$ and $s_r$ with different probability, and both
are terminal states.

* Let $n$ index the current trial and $\hat{V}_{n}(s)$ be
the state-value function estimate on trial $n$ of state
$s\in\{s_l,s_r\}$. In the 2-armed bandit task descirbed
above, TD iteratively updates its estimate of
$\hat{V}_{n}(s)$ according to the following:

$$
\begin{equation}
\hat{V}_{n}(s) = \hat{V}_{n-1}(s) + \alpha (r_{n} - \hat{V}_{n-1}(s)).
\end{equation}
$$

* The rightmost term $r_{n} - \hat{V}_{n-1}(s)$ is called
the reward prediction error (RPE).

* Conceptually, RPE is simply the difference between the
obtained and expected reward.

* It is easy to see that learning a good estimate of the
value function is equivalent to eliminating RPE.

* RPE is often notated as $\delta$, so we can write
$\delta_{n}=r_{n}-\hat{V}_{n-1}(s)$.

* You can also write the value update equation in the
following form:

$$
\begin{equation}
\hat{V}_{n}(s) =  (1-\alpha) \hat{V}_{n-1}(s) +  \alpha r_{n}.
\end{equation}
$$

* In this form it may be easier to see that the update to
our estimate of the state-value function is a weighted
average of whatever it was on the previous trial with
whatever current reward was experienced.

* In code, a TD agent performing a 2-armed bandit task in
which it simply chooses which bandit to select at random
looks as follows:

```{python}
import numpy as np
import matplotlib.pyplot as plt

n_trials = 1000

v_init = 0.5
p_reward_1 = 7
p_reward_2 = 6
alpha = 0.01
epsilon = 0.2

v = np.zeros((2, n_trials))
v[:, 0] = v_init

for i in range(0, n_trials - 1):

    # action selection - guessing
    if np.random.uniform() < 0.5:
        # reward
        r = np.random.normal(p_reward_1, 2)

        # reward prediction error
        delta = r - v[0, i]

        # value update
        v[0, i + 1] = v[0, i] + alpha * delta
        v[1, i + 1] = v[1, i]

    else:
        # reward
        r = np.random.normal(p_reward_2, 2)

        # reward prediction error
        delta = r - v[1, i]

        # value update
        v[1, i + 1] = v[1, i] + alpha * delta
        v[0, i + 1] = v[0, i]

fig, ax = plt.subplots(1, 1, squeeze=False)
ax[0, 0].plot(v[0, :], label='value 1')
ax[0, 0].plot(v[1, :], label='value 2')
plt.legend()
plt.show()
```

### Action selection policy

* We saw above that even if the agent simply guesses at each
bandit, never modifying its action selection strategy to
reflect its updating beliefs about the value of the the two
options, the estimate of the value function still approaches
the true value.

* This makes clear that some amount of guessing (i.e.,
exploration) is good for learning the value function, but
perhaps not so great for actually maximising the obtained
rewards (the actual goal of an RL agent).

* Two popular action selection policies -- epsilon greedy
($\epsilon$-greedy) and softman -- attempt to balance
exploration with exploitation.

#### Epsilon greedy

```{python}
import numpy as np
import matplotlib.pyplot as plt

n_trials = 1000

v_init = 0.5
p_reward_1 = 7
p_reward_2 = 6
alpha = 0.01
epsilon = 0.2

v = np.zeros((2, n_trials))
v[:, 0] = v_init

for i in range(0, n_trials - 1):

    # action selection - greedy epsilon
    if np.random.uniform() < epsilon:
        a = np.round(np.random.uniform())
    else:
        a = np.argmax(v[:, i])

    if a == 0:
        # reward
        r = np.random.normal(p_reward_1, 2)

        # reward prediction error
        delta = r - v[0, i]

        # value update
        v[0, i + 1] = v[0, i] + alpha * delta
        v[1, i + 1] = v[1, i]

    else:
        # reward
        r = np.random.normal(p_reward_2, 2)

        # reward prediction error
        delta = r - v[1, i]

        # value update
        v[1, i + 1] = v[1, i] + alpha * delta
        v[0, i + 1] = v[0, i]

fig, ax = plt.subplots(1, 1, squeeze=False)
ax[0, 0].plot(v[0, :], label='value 1')
ax[0, 0].plot(v[1, :], label='value 2')
plt.legend()
plt.show()
```

#### Softmax

```{python}
import numpy as np
import matplotlib.pyplot as plt

n_trials = 1000

v_init = 0.5
p_reward_1 = 7
p_reward_2 = 6
alpha = 0.01
epsilon = 0.2

v = np.zeros((2, n_trials))
v[:, 0] = v_init

for i in range(0, n_trials - 1):

    # action selection - softmax
    sm = np.exp(v[:, i]) / np.sum(np.exp(v[:, i]))
    if np.random.uniform() < sm[0]:
        # reward
        r = np.random.normal(p_reward_1, 2)

        # reward prediction error
        delta = r - v[0, i]

        # value update
        v[0, i + 1] = v[0, i] + alpha * delta
        v[1, i + 1] = v[1, i]

    else:
        # reward
        r = np.random.normal(p_reward_2, 2)

        # reward prediction error
        delta = r - v[1, i]

        # value update
        v[1, i + 1] = v[1, i] + alpha * delta
        v[0, i + 1] = v[0, i]

fig, ax = plt.subplots(1, 1, squeeze=False)
ax[0, 0].plot(v[0, :], label='value 1')
ax[0, 0].plot(v[1, :], label='value 2')
plt.legend()
plt.show()
```


## SARSA for estimating $Q$

* SARSA -- state-action-reward-state-action -- attempts to
learn $Q(s, a)$, called the action-value function, which
represents the value (or *quality* hence the $Q$) of taking
action $a$ in state $s$.

* The policy that controls behaviour is derived from $Q$ and
that makes SARSA about both *prediction* and *control*.

* SARSA is called **on policy** because the only $Q$ values
that are learned about are those that correspond to
state-action pairs that were directly experienced.

***

<div style="border: 1px solid black; background: lightgrey;">

* Iterate over episodes
  
  * Initialise $S$
  
  * Choose $A$ from $S$ using policy derived from $Q$ (e.g.,
  $\epsilon$-greedy)

  * Iterate over steps per episodes

    * Take action $A$, observe $R$, $S'$
    
    * Choose $A'$ from $S'$ using policy derived from $Q$ (e.g.,
    $\epsilon$-greedy)
    
    * $Q(S, A) \leftarrow Q(S, A) + \alpha \left[ R + \gamma Q(S', A') - Q(S, A) \right]$
    
    * $S \leftarrow S'$, $A \leftarrow A'$
    
    * If $S = S_{\text{terminal}}$ then break 

</div>

***

## Q-learning for estimating $\pi$

* Like SARSA, Q-learning attempts to learn $Q(s, a)$.

* Again like SARSA, Q-learning is about both *prediction*
and *control*.

* Q-learning is called **off policy** because the $Q$ values
that are learned about are not necessarily only those that
correspond to state-action pairs that were directly
experienced.

***

<div style="border: 1px solid black; background: lightgrey;">

* Initialise $Q(s, a)$

* Iterate over episodes

  * Initialise $S$

  * Iterate over steps per episodes

    * Choose $A$ from $S$ using policy derived from $Q$ (e.g.,
    $\epsilon$-greedy)
    
    * Take action $A$, observe $R$, $S'$
    
    * $Q(S, A) \leftarrow Q(S, A) + \alpha \left[ R + \gamma \max_{a} Q(S', a) - Q(S, A) \right]$
    
    * $S \leftarrow S'$
    
    * If $S = S_{\text{terminal}}$ then break 

</div>

***

### Q-learning applied to instrumental conditioning

```{python}
import numpy as np
import matplotlib.pyplot as plt

n_episodes = 100
n_steps = 10

n_states = 6
n_actions = 3

alpha = 0.1

# initialise q(s,a)
q = np.ones((n_states, n_actions)) * 0.5

# iterate over episodes
for e in range(n_episodes):

    # initialise s
    s = 0

    # iterate over steps per episodes
    for t in range(n_steps):

        # choose a from s using policy derived from q
        # here, we use softmax
        sm = np.exp(q[s, :]) / np.sum(np.exp(q[s, :]))
        a = np.random.choice([0, 1, 2], size=1, p=np.squeeze(sm))

        # take action a, observe r, s'

        if s==0:
            # press lever
            if a == 0:
                r = 0
                sprime = 1

            # pull chain
            elif a == 1:
                r = 0
                sprime = 2

            # enter magazine
            elif a == 2:
                r = 0
                sprime = 3

        elif s==1:
            # press lever
            if a == 0:
                r = 0
                sprime = 3

            # pull chain
            elif a == 1:
                r = 0
                sprime = 3

            # enter magazine
            elif a == 2:
                r = 1
                sprime = 4

        elif s==2:
            # press lever
            if a == 0:
                r = 0
                sprime = 3

            # pull chain
            elif a == 1:
                r = 0
                sprime = 3

            # enter magazine
            elif a == 2:
                r = 1
                sprime = 5

        # update q-function
        q[s, a] += alpha * (r + np.max(q[sprime, :]) - q[s, a])

        # reset state
        s = sprime

        # stop if s is terminal
        if s == 3 or s == 4 or s == 5:
            break


fig, ax = plt.subplots(1, 1, squeeze=False)
ax[0,0].imshow(q)
ax[0, 0].set_xlabel('action')
ax[0, 0].set_ylabel('state')
plt.show()
```

### Q-learning applied to instrumental conditioning 2

```{python}
import numpy as np
import matplotlib.pyplot as plt

n_episodes = 100
n_steps = 10

n_states = 6
n_actions = 3

alpha = 0.1

# initialise q(s,a)
q = np.ones((n_states, n_actions)) * 0.5

# states
S = np.arange(0, 6, 1)

# Actions
A = np.array([0, 1, 2])

# state transition probabilities
T = np.zeros((n_states, n_actions, n_states))

T[0, 0, 1] = 1 # press lever transition to state 1
T[0, 1, 2] = 1 # pull chain transition to state 2
T[0, 2, 3] = 1 # enter magazine terminal no reward

T[1, 0, 3] = 1 # press lever terminal no reward
T[1, 1, 3] = 1 # pull chain terminal no reward
T[1, 2, 4] = 1 # enter magazine terminal reward

T[2, 0, 3] = 1 # press lever terminal no reward
T[2, 1, 3] = 1 # pull chain terminal no reward
T[2, 2, 5] = 1 # enter magazine terminal reward

# state rewards
R = np.zeros(n_states)
R[4] = 1
R[5] = 1

# iterate over episodes
for e in range(n_episodes):

    # initialise s
    s = 0

    # iterate over steps per episodes
    for t in range(n_steps):

        # choose a from s using policy derived from q
        # here, we use softmax
        sm = np.exp(q[s, :]) / np.sum(np.exp(q[s, :]))
        a = np.random.choice(A, size=1, p=np.squeeze(sm))

        # take action a, observe r, s'
        sprime = np.random.choice(S, size=1, p=np.squeeze(T[s, a, :]))
        r = R[sprime]

        # update q-function
        q[s, a] += alpha * (r + np.max(q[sprime, :]) - q[s, a])

        # reset state
        s = sprime

        # stop if s is terminal
        if s == 3 or s == 4 or s == 5:
            break


fig, ax = plt.subplots(1, 1, squeeze=False)
ax[0,0].imshow(q)
ax[0, 0].set_xlabel('action')
ax[0, 0].set_ylabel('state')
plt.show()
```

## Dyna-Q: Model-based RL

***

<div style="border: 1px solid black; background: lightgrey;">

* Initialise $Q(s, a)$, $\widehat{Q}(s, a)$, and
$\widehat{R}(s)$

* Iterate over episodes

  * Initialise $S$

  * Iterate over steps per episodes

    * Choose $A$ from $S$ using policy derived from $Q$ (e.g.,
    $\epsilon$-greedy)
    
    * Take action $A$, observe $R$, $S'$
    
    * $Q(S, A) \leftarrow Q(S, A) + \alpha \left[ R + \gamma \max_{a} Q(S', a) - Q(S, A) \right]$
    
    * Iterate over model-based episodes
    
      * $S \leftarrow$ random previously observed state
      
      * $A \leftarrow$ random previously taken action from state $S$
      
      * $S' \leftarrow$ sampled with probability $\widehat{T}(S, A, S')$
      
      * $R \leftarrow \widehat{R}(S)$  
      
      * $Q(S, A) \leftarrow Q(S, A) + \alpha \left[ R + \gamma \max_{a} Q(S', a) - Q(S, A) \right]$
    
    * $S \leftarrow S'$
    
    * If $S = S_{\text{terminal}}$ then break 

</div>

***

### Dyna-Q applied to instrumental conditioning

```{python}
import numpy as np
import matplotlib.pyplot as plt

n_episodes = 100
n_steps = 10

n_states = 6
n_actions = 3

alpha = 0.1

# initialise q(s,a)
q = np.ones((n_states, n_actions)) * 0.5

# states
S = np.arange(0, 6, 1)

# Actions
A = np.array([0, 1, 2])

# state transition probabilities
T = np.zeros((n_states, n_actions, n_states))

T[0, 0, 1] = 1  # press lever transition to state 1
T[0, 1, 2] = 1  # pull chain transition to state 2
T[0, 2, 3] = 1  # enter magazine terminal no reward

T[1, 0, 3] = 1  # press lever terminal no reward
T[1, 1, 3] = 1  # pull chain terminal no reward
T[1, 2, 4] = 1  # enter magazine terminal reward

T[2, 0, 3] = 1  # press lever terminal no reward
T[2, 1, 3] = 1  # pull chain terminal no reward
T[2, 2, 5] = 1  # enter magazine terminal reward

# state rewards
R = np.zeros(n_states)
R[4] = 1
R[5] = 1

# model of the environment
n = 10
T_hat = np.zeros((n_states, n_actions, n_states))
R_hat = np.zeros(n_states)
S_past = np.array([])
A_past = np.ones((n_states, n_actions)) * -1

# iterate over episodes
for e in range(n_episodes):

    # initialise s
    s = 0

    # iterate over steps per episodes
    for t in range(n_steps):

        # choose a from s using policy derived from q
        # here, we use softmax
        sm = np.exp(q[s, :]) / np.sum(np.exp(q[s, :]))
        a = np.random.choice(A, size=1, p=np.squeeze(sm))[0]

        # take action a, observe r, s'
        sprime = np.random.choice(S, size=1, p=np.squeeze(T[s, a, :]))[0]
        r = R[sprime]

        # update q-function
        q[s, a] += alpha * (r + np.max(q[sprime, :]) - q[s, a])

        # update models of the environment (tabular Dyna-Q p. 164)
        # assuming deterministic environment
        T_hat[s, a, sprime] = 1
        R_hat[sprime] = r

        # keep track of experienced states and actions
        S_past = np.append(S_past, [s])
        S_past = np.unique(S_past)
        A_past[s, a] = 1

        # Simulate experience
        for i in range(n):
            # pick a previously experienced state
            s = np.random.choice(S_past, size=1)[0].astype(int)

            # select an action previously taken from state s
            eligible_actions = A_past[s, :] == 1
            a = np.random.choice(np.where(eligible_actions)[0], size=1)[0]

            # simulate the outcome
            sprime_sim = np.random.choice(S,
                                          size=1,
                                          p=np.squeeze(T_hat[s, a, :]))[0]
            r = R[sprime_sim]

            # update the real Q function on the basis of the simulated outcome
            q[s, a] += alpha * (r + np.max(q[sprime_sim, :]) - q[s, a])

        # reset state
        s = sprime

        # stop if s is terminal
        if s == 3 or s == 4 or s == 5:
            break

fig, ax = plt.subplots(1, 1, squeeze=False)
ax[0, 0].imshow(q)
ax[0, 0].set_xlabel('action')
ax[0, 0].set_ylabel('state')
plt.show()
```


<!-- ## TD sequential decision making task -->

<!-- * **NOTE: ** This example is starting to encroach on -->
<!-- something called Q-learning, which is like TD learning but -->
<!-- explicitly incorporates actions into the value function. I -->
<!-- will cover Q learning next week and make clear its -->
<!-- relationship to this example. -->

<!-- * Consider a task in which the agent must first choose -->
<!-- between two bandits, but then, rather than being transported -->
<!-- to a terminal state as in the simple 2-arm bandit task, is -->
<!-- instead transported to one of two new states in which the -->
<!-- agent must choose between two new bandits. Reward is again -->
<!-- given only in the terminal states. The possible state -->
<!-- transitions are given by the following: -->

<!-- $$ -->
<!-- s_{l} \rightarrow s \in \{s_{ll}, s_{lr}\} \\ -->
<!-- s_{r} \rightarrow s \in \{s_{rl}, s_{rr}\} -->
<!-- $$ -->

<!-- * In code, a TD agent performing a sequential decision -->
<!-- making task looks as follows: -->

<!-- ```{python} -->
<!-- import numpy as np -->
<!-- import matplotlib.pyplot as plt -->

<!-- n_trials = 1000 -->

<!-- v_init = 0.5 -->

<!-- p_reward = [0.0, 0.0, 0.0, 50, 100, 25, 150] -->

<!-- alpha = 0.01 -->
<!-- gamma = 0.1 -->
<!-- epsilon = 0.2 -->

<!-- v = np.zeros((7, n_trials)) -->
<!-- v[:, 0] = v_init -->


<!-- for i in range(0, n_trials - 1): -->

<!--     s_trace = np.zeros(3) -->

<!--     # step 0 -->
<!--     s = 0 -->
<!--     sprime_set = [1, 2] -->
<!--     sm = np.exp(v[sprime_set, i]) / np.sum(np.exp(v[sprime_set, i])) -->
<!--     a = (np.random.uniform() < sm[0]).astype(int) -->
<!--     sprime = sprime_set[a] -->
<!--     r = 0 -->
<!--     v[s, i + 1] = v[s, i] + alpha * (r + gamma * v[sprime, i] - v[s, i]) -->
<!--     s_trace[1] = sprime -->

<!--     # step 1 -->
<!--     s = sprime -->
<!--     sprime_set = [3, 4] if s == 1 else [5, 6] -->
<!--     sm = np.exp(v[sprime_set, i]) / np.sum(np.exp(v[sprime_set, i])) -->
<!--     a = (np.random.uniform() < sm[0]).astype(int) -->
<!--     sprime = sprime_set[a] -->
<!--     r = 0 -->
<!--     v[s, i + 1] = v[s, i] + alpha * (r + gamma * v[sprime, i] - v[s, i]) -->
<!--     s_trace[2] = sprime -->

<!--     # step 2 -->
<!--     s = sprime -->
<!--     r = np.random.normal(p_reward[sprime], 2) -->
<!--     v[s, i + 1] = v[s, i] + alpha * (r - v[s, i]) -->

<!--     nots = np.setdiff1d(np.arange(0, 7, 1), s_trace) -->
<!--     v[nots, i + 1] = v[nots, i] -->

<!-- fig, ax = plt.subplots(1, 3, squeeze=False) -->
<!-- ax[0, 0].plot(v[0, :], label='value 0') -->

<!-- ax[0, 1].plot(v[1, :], label='value 1') -->
<!-- ax[0, 1].plot(v[2, :], label='value 2') -->

<!-- ax[0, 2].plot(v[3, :], label='value 3') -->
<!-- ax[0, 2].plot(v[4, :], label='value 4') -->
<!-- ax[0, 2].plot(v[5, :], label='value 5') -->
<!-- ax[0, 2].plot(v[6, :], label='value 6') -->

<!-- ax[0, 0].legend() -->
<!-- ax[0, 1].legend() -->
<!-- ax[0, 2].legend() -->
<!-- plt.show() -->
<!-- ``` -->
